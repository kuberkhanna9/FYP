{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NASDAQ-100 Sector-Based Feature Selection using Shannon Mutual Information\n",
        "\n",
        "This notebook performs comprehensive feature importance analysis across all NASDAQ-100 stocks using Shannon Mutual Information (SMI), with a focus on sector-specific patterns and cross-sector relationships.\n",
        "\n",
        "## Process:\n",
        "1. Load validated sector-labeled dataset\n",
        "2. Calculate SMI scores for each stock\n",
        "3. Aggregate and analyze by sector\n",
        "4. Identify sector-specific patterns\n",
        "5. Generate feature importance rankings\n",
        "6. Export results and visualizations\n",
        "\n",
        "## Analysis Components:\n",
        "\n",
        "1. **Stock-Level Analysis**\n",
        "   - Individual feature importance\n",
        "   - Stock-specific patterns\n",
        "   - Temporal consistency\n",
        "   - Sector alignment\n",
        "\n",
        "2. **Sector-Level Analysis**\n",
        "   - Sector-specific important features\n",
        "   - Common patterns within sectors\n",
        "   - Cross-sector differences\n",
        "   - Sector characteristic indicators\n",
        "\n",
        "3. **Feature Group Analysis**\n",
        "   - Price-based features\n",
        "   - Technical indicators\n",
        "   - Volatility metrics\n",
        "   - Volume indicators\n",
        "   - Sector-relative features\n",
        "\n",
        "4. **Cross-Sectional Analysis**\n",
        "   - Feature importance distributions\n",
        "   - Sector-wise correlations\n",
        "   - Feature redundancy patterns\n",
        "   - Temporal stability\n",
        "\n",
        "5. **Feature Selection Strategy**\n",
        "   - Universal vs. sector-specific features\n",
        "   - Redundancy elimination\n",
        "   - Robustness validation\n",
        "   - Feature set optimization\n",
        "\n",
        "## Key Considerations:\n",
        "- Feature normalization per stock/sector\n",
        "- Class imbalance handling\n",
        "- Temporal dependencies\n",
        "- Sector-specific characteristics\n",
        "- Computational efficiency\n",
        "- Feature stability\n",
        "- Selection criteria\n",
        "\n",
        "## Output:\n",
        "- Feature importance scores: `/data/smi_scores/{SECTOR}.csv`\n",
        "- Stock-level rankings: `/data/smi_scores/{TICKER}.csv`\n",
        "- Sector analysis: `/reports/sector_feature_analysis.txt`\n",
        "- Visualizations: `/reports/figures/features/`\n",
        "- Summary report: `/reports/feature_selection_summary.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-27 22:31:41,208 - INFO - Configuration complete. Using 19 processes for parallel computation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Groups:\n",
            "- Price (5 features): Raw price and volume data\n",
            "- Technical (9 features): Technical analysis indicators\n",
            "- Volatility (3 features): Volatility and return metrics\n",
            "- Volume (3 features): Volume-based indicators\n",
            "- Sector (3 features): Sector-relative indicators\n",
            "\n",
            "Total features: 23\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import multiprocessing\n",
        "from scipy import stats\n",
        "import networkx as nx\n",
        "\n",
        "# Configure logging\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = f'../reports/feature_selection_{timestamp}.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set_palette(\"husl\", n_colors=11)  # One color per sector\n",
        "\n",
        "# Customize plot appearance\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': [15, 8],\n",
        "    'axes.grid': True,\n",
        "    'axes.grid.which': 'both',\n",
        "    'axes.grid.axis': 'both',\n",
        "    'axes.facecolor': 'white',\n",
        "    'figure.facecolor': 'white',\n",
        "    'grid.color': '#CCCCCC',\n",
        "    'grid.alpha': 0.5,\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 10,\n",
        "    'legend.frameon': True,\n",
        "    'legend.framealpha': 0.8,\n",
        "    'axes.spines.left': True,\n",
        "    'axes.spines.bottom': True,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.top': False\n",
        "})\n",
        "\n",
        "# Create necessary directories\n",
        "for dir_path in [\n",
        "    '../data/smi_scores',\n",
        "    '../reports/figures/features',\n",
        "    '../reports/figures/sectors'\n",
        "]:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set number of processes for parallel computation\n",
        "N_PROCESSES = max(1, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "# Define feature groups with descriptions\n",
        "FEATURE_GROUPS = {\n",
        "    'Price': {\n",
        "        'features': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "        'description': 'Raw price and volume data'\n",
        "    },\n",
        "    'Technical': {\n",
        "        'features': [\n",
        "            'EMA_10', 'EMA_50',  # Trend following (renamed from Short/Long)\n",
        "            'RSI',               # Momentum\n",
        "            'MACD', 'MACD_Signal',  # Trend and momentum\n",
        "            'MACD_Hist',         # Momentum divergence\n",
        "            'BB_Upper', 'BB_Lower',  # Volatility bands\n",
        "            'BB_Width'          # Volatility measure\n",
        "        ],\n",
        "        'description': 'Technical analysis indicators'\n",
        "    },\n",
        "    'Volatility': {\n",
        "        'features': [\n",
        "            'Volatility',        # Historical volatility\n",
        "            'Daily_Return',      # Daily returns\n",
        "            'Log_Return'         # Log returns\n",
        "        ],\n",
        "        'description': 'Volatility and return metrics'\n",
        "    },\n",
        "    'Volume': {\n",
        "        'features': [\n",
        "            'OBV',              # On-Balance Volume\n",
        "            'Volume_MA',        # Volume moving average\n",
        "            'Volume_Ratio'      # Current to average volume\n",
        "        ],\n",
        "        'description': 'Volume-based indicators'\n",
        "    },\n",
        "    'Sector': {\n",
        "        'features': [\n",
        "            'Relative_Strength',  # Relative strength vs sector\n",
        "            'Sector_Return',     # Sector returns\n",
        "            'RS_MA'             # Moving average of relative strength\n",
        "        ],\n",
        "        'description': 'Sector-relative indicators'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Feature selection configuration\n",
        "FEATURE_CONFIG = {\n",
        "    'min_samples': 100,        # Minimum samples required\n",
        "    'min_class_pct': 5.0,      # Minimum class percentage\n",
        "    'test_size': 0.2,          # Validation set size\n",
        "    'cv_folds': 5,             # Cross-validation folds\n",
        "    'smi_random_state': 42,    # Random state for reproducibility\n",
        "    'correlation_threshold': 0.7  # Threshold for feature correlation\n",
        "}\n",
        "\n",
        "# Sector colors for consistent plotting\n",
        "SECTOR_COLORS = {\n",
        "    'Information Technology': '#2ecc71',\n",
        "    'Healthcare': '#3498db',\n",
        "    'Financials': '#9b59b6',\n",
        "    'Consumer Discretionary': '#e74c3c',\n",
        "    'Consumer Staples': '#f1c40f',\n",
        "    'Energy': '#e67e22',\n",
        "    'Industrials': '#1abc9c',\n",
        "    'Materials': '#34495e',\n",
        "    'Real Estate': '#95a5a6',\n",
        "    'Utilities': '#7f8c8d',\n",
        "    'Communication Services': '#d35400'\n",
        "}\n",
        "\n",
        "# Flatten feature list and add descriptions\n",
        "FEATURE_COLUMNS = []\n",
        "FEATURE_DESCRIPTIONS = {}\n",
        "for group_name, group_info in FEATURE_GROUPS.items():\n",
        "    FEATURE_COLUMNS.extend(group_info['features'])\n",
        "    for feature in group_info['features']:\n",
        "        FEATURE_DESCRIPTIONS[feature] = {\n",
        "            'group': group_name,\n",
        "            'description': group_info['description']\n",
        "        }\n",
        "\n",
        "logging.info(f\"Configuration complete. Using {N_PROCESSES} processes for parallel computation.\")\n",
        "print(f\"\\nFeature Groups:\")\n",
        "for group, info in FEATURE_GROUPS.items():\n",
        "    print(f\"- {group} ({len(info['features'])} features): {info['description']}\")\n",
        "print(f\"\\nTotal features: {len(FEATURE_COLUMNS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-27 22:31:41,253 - INFO - Found 109 tickers in sector mapping\n",
            "Loading stock data: 100%|██████████| 108/108 [00:03<00:00, 34.50it/s]\n",
            "2025-07-27 22:32:02,087 - INFO - Combined dataset saved to ..\\data\\enriched\\nasdaq_features.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Loading:\n",
            "- Total records: 266,029\n",
            "- Date range: 2015-01-02 to 2025-07-25\n",
            "\n",
            "Warning: Missing data for tickers:\n",
            "- ATVI (Information Technology)\n",
            "\n",
            "Warning: Missing sectors:\n",
            "- Materials \n",
            "\n",
            "Feature Availability:\n",
            "\n",
            "Price Features:\n",
            "- Available (5/5):\n",
            "  * Open\n",
            "  * High\n",
            "  * Low\n",
            "  * Close\n",
            "  * Volume\n",
            "\n",
            "Technical Features:\n",
            "- Available (9/9):\n",
            "  * EMA_10\n",
            "  * EMA_50\n",
            "  * RSI\n",
            "  * MACD\n",
            "  * MACD_Signal\n",
            "  * MACD_Hist\n",
            "  * BB_Upper\n",
            "  * BB_Lower\n",
            "  * BB_Width\n",
            "\n",
            "Volatility Features:\n",
            "- Available (3/3):\n",
            "  * Volatility\n",
            "  * Daily_Return\n",
            "  * Log_Return\n",
            "\n",
            "Volume Features:\n",
            "- Available (3/3):\n",
            "  * OBV\n",
            "  * Volume_MA\n",
            "  * Volume_Ratio\n",
            "\n",
            "Sector Features:\n",
            "- Available (3/3):\n",
            "  * Relative_Strength\n",
            "  * Sector_Return\n",
            "  * RS_MA\n",
            "\n",
            "Data Quality Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating stocks: 100%|██████████| 108/108 [00:02<00:00, 43.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quality Summary:\n",
            "- Stocks: 108 across 11 sectors\n",
            "- Average days per stock: 2463.2\n",
            "- Minimum days per stock: 929\n",
            "- Average missing values: 0.60%\n",
            "\n",
            "Stocks with Quality Issues:\n",
            "- AAPL:\n",
            "  * Missing values: max 0.8%\n",
            "- ABNB:\n",
            "  * Missing values: max 1.7%\n",
            "- ADBE:\n",
            "  * Missing values: max 0.8%\n",
            "- ADI:\n",
            "  * Missing values: max 0.8%\n",
            "- ADP:\n",
            "  * Missing values: max 0.8%\n",
            "- ADSK:\n",
            "  * Missing values: max 0.8%\n",
            "- AEP:\n",
            "  * Missing values: max 0.8%\n",
            "- ALGN:\n",
            "  * Missing values: max 7.5%\n",
            "- AMAT:\n",
            "  * Missing values: max 0.8%\n",
            "- AMD:\n",
            "  * Missing values: max 0.8%\n",
            "- AMGN:\n",
            "  * Missing values: max 0.8%\n",
            "- AMZN:\n",
            "  * Missing values: max 0.8%\n",
            "- ANSS:\n",
            "  * Missing values: max 0.8%\n",
            "- APD:\n",
            "  * Missing values: max 0.8%\n",
            "- ASML:\n",
            "  * Missing values: max 0.8%\n",
            "- AVGO:\n",
            "  * Missing values: max 14.1%\n",
            "- BIIB:\n",
            "  * Missing values: max 1.7%\n",
            "- BKNG:\n",
            "  * Missing values: max 10.6%\n",
            "- BKR:\n",
            "  * Missing values: max 0.8%\n",
            "- CDNS:\n",
            "  * Missing values: max 0.8%\n",
            "- CHTR:\n",
            "  * Missing values: max 33.6%\n",
            "- CMCSA:\n",
            "  * Missing values: max 33.6%\n",
            "- COIN:\n",
            "  * Missing values: max 1.9%\n",
            "- COST:\n",
            "  * Missing values: max 0.8%\n",
            "- CPRT:\n",
            "  * Missing values: max 0.8%\n",
            "- CRWD:\n",
            "  * Missing values: max 1.3%\n",
            "- CSCO:\n",
            "  * Missing values: max 0.8%\n",
            "- CSGP:\n",
            "  * Missing values: max 8.0%\n",
            "- CSX:\n",
            "  * Missing values: max 0.8%\n",
            "- CTAS:\n",
            "  * Missing values: max 0.8%\n",
            "- CTSH:\n",
            "  * Missing values: max 0.8%\n",
            "- CVNA:\n",
            "  * Missing values: max 1.0%\n",
            "- DASH:\n",
            "  * Missing values: max 1.7%\n",
            "- DDOG:\n",
            "  * Missing values: max 1.4%\n",
            "- DLTR:\n",
            "  * Missing values: max 0.8%\n",
            "- DOCU:\n",
            "  * Missing values: max 11.5%\n",
            "- DXCM:\n",
            "  * Missing values: max 0.8%\n",
            "- EA:\n",
            "  * Missing values: max 0.8%\n",
            "- EBAY:\n",
            "  * Missing values: max 0.8%\n",
            "- ENPH:\n",
            "  * Missing values: max 0.8%\n",
            "- EQIX:\n",
            "  * Missing values: max 8.0%\n",
            "- EXC:\n",
            "  * Missing values: max 0.8%\n",
            "- FANG:\n",
            "  * Missing values: max 0.8%\n",
            "- FAST:\n",
            "  * Missing values: max 0.8%\n",
            "- FCX:\n",
            "  * Missing values: max 0.8%\n",
            "- FTNT:\n",
            "  * Missing values: max 0.8%\n",
            "- GFS:\n",
            "  * Missing values: max 2.1%\n",
            "- GILD:\n",
            "  * Missing values: max 3.2%\n",
            "- GOOGL:\n",
            "  * Missing values: max 0.8%\n",
            "- GOOG:\n",
            "  * Missing values: max 0.8%\n",
            "- HON:\n",
            "  * Missing values: max 0.8%\n",
            "- IDXX:\n",
            "  * Missing values: max 0.8%\n",
            "- ILMN:\n",
            "  * Missing values: max 0.8%\n",
            "- INTC:\n",
            "  * Missing values: max 0.8%\n",
            "- INTU:\n",
            "  * Missing values: max 0.8%\n",
            "- ISRG:\n",
            "  * Missing values: max 0.8%\n",
            "- KDP:\n",
            "  * Missing values: max 0.8%\n",
            "- KHC:\n",
            "  * Missing values: max 0.8%\n",
            "- KLAC:\n",
            "  * Missing values: max 1.0%\n",
            "- LCID:\n",
            "  * Missing values: max 1.6%\n",
            "- LIN:\n",
            "  * Missing values: max 0.8%\n",
            "- LRCX:\n",
            "  * Missing values: max 0.8%\n",
            "- MAR:\n",
            "  * Missing values: max 2.2%\n",
            "- MCHP:\n",
            "  * Missing values: max 0.8%\n",
            "- MDLZ:\n",
            "  * Missing values: max 0.8%\n",
            "- MELI:\n",
            "  * Missing values: max 0.8%\n",
            "- META:\n",
            "  * Missing values: max 6.5%\n",
            "- MNST:\n",
            "  * Missing values: max 0.8%\n",
            "- MRNA:\n",
            "  * Missing values: max 3.2%\n",
            "- MRVL:\n",
            "  * Missing values: max 0.8%\n",
            "- MSFT:\n",
            "  * Missing values: max 0.8%\n",
            "- MTCH:\n",
            "  * Missing values: max 2.4%\n",
            "- MU:\n",
            "  * Missing values: max 0.8%\n",
            "- NFLX:\n",
            "  * Missing values: max 33.6%\n",
            "- NVDA:\n",
            "  * Missing values: max 14.1%\n",
            "- NXPI:\n",
            "  * Missing values: max 0.8%\n",
            "- ODFL:\n",
            "  * Missing values: max 0.8%\n",
            "- OKTA:\n",
            "  * Missing values: max 3.3%\n",
            "- ON:\n",
            "  * Missing values: max 0.8%\n",
            "- ORLY:\n",
            "  * Missing values: max 0.8%\n",
            "- PANW:\n",
            "  * Missing values: max 2.0%\n",
            "- PAYX:\n",
            "  * Missing values: max 0.8%\n",
            "- PCAR:\n",
            "  * Missing values: max 4.3%\n",
            "- PEP:\n",
            "  * Missing values: max 0.8%\n",
            "- PLD:\n",
            "  * Missing values: max 8.0%\n",
            "- PYPL:\n",
            "  * Missing values: max 14.0%\n",
            "- QCOM:\n",
            "  * Missing values: max 0.8%\n",
            "- RBLX:\n",
            "  * Missing values: max 1.8%\n",
            "- REGN:\n",
            "  * Missing values: max 1.1%\n",
            "- RIVN:\n",
            "  * Missing values: max 2.2%\n",
            "- ROKU:\n",
            "  * Missing values: max 12.7%\n",
            "- ROST:\n",
            "  * Missing values: max 0.8%\n",
            "- SBUX:\n",
            "  * Missing values: max 0.8%\n",
            "- SIRI:\n",
            "  * Missing values: max 33.6%\n",
            "- SNAP:\n",
            "  * Missing values: max 14.9%\n",
            "- SNPS:\n",
            "  * Missing values: max 0.8%\n",
            "- TEAM:\n",
            "  * Missing values: max 0.8%\n",
            "- TMUS:\n",
            "  * Missing values: max 33.6%\n",
            "- TSLA:\n",
            "  * Missing values: max 0.8%\n",
            "- TTD:\n",
            "  * Missing values: max 0.9%\n",
            "- TXN:\n",
            "  * Missing values: max 0.8%\n",
            "- VRSK:\n",
            "  * Missing values: max 0.8%\n",
            "- VRTX:\n",
            "  * Missing values: max 0.8%\n",
            "- WBD:\n",
            "  * Missing values: max 33.6%\n",
            "- WDAY:\n",
            "  * Missing values: max 0.8%\n",
            "- XEL:\n",
            "  * Missing values: max 0.8%\n",
            "- ZM:\n",
            "  * Missing values: max 6.4%\n",
            "- ZS:\n",
            "  * Missing values: max 1.1%\n",
            "\n",
            "Detailed quality report saved to: ../reports/data_quality_20250727_223141.txt\n",
            "\n",
            "Ready to process 108 stocks across 11 sectors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare dataset\n",
        "try:\n",
        "    # Load sector mapping for reference\n",
        "    sector_mapping = pd.read_csv('../data/sector_mapping.csv')\n",
        "    all_tickers = sector_mapping['Ticker'].unique()\n",
        "    logging.info(f\"Found {len(all_tickers)} tickers in sector mapping\")\n",
        "    \n",
        "    # Load and combine enriched data from stocks directory\n",
        "    stock_files = list(Path('../data/enriched/stocks').glob('*_features.csv'))\n",
        "    if not stock_files:\n",
        "        raise FileNotFoundError(\"No feature files found. Please run feature engineering notebook first.\")\n",
        "    \n",
        "    # Load all stock data\n",
        "    stock_data = []\n",
        "    for file in tqdm(stock_files, desc=\"Loading stock data\"):\n",
        "        if 'statistics' not in file.name:  # Skip statistics file\n",
        "            df = pd.read_csv(file)\n",
        "            stock_data.append(df)\n",
        "    \n",
        "    # Combine all stock data\n",
        "    df = pd.concat(stock_data, ignore_index=True)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    \n",
        "    # Save combined dataset\n",
        "    output_path = Path('../data/enriched/nasdaq_features.csv')\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    logging.info(f\"Combined dataset saved to {output_path}\")\n",
        "    \n",
        "    # Verify data loading\n",
        "    print(\"\\nDataset Loading:\")\n",
        "    print(f\"- Total records: {len(df):,}\")\n",
        "    print(f\"- Date range: {df['Date'].min():%Y-%m-%d} to {df['Date'].max():%Y-%m-%d}\")\n",
        "    \n",
        "    # Check for missing tickers\n",
        "    available_tickers = df['Ticker'].unique()\n",
        "    missing_tickers = set(all_tickers) - set(available_tickers)\n",
        "    if missing_tickers:\n",
        "        print(\"\\nWarning: Missing data for tickers:\")\n",
        "        for ticker in sorted(missing_tickers):\n",
        "            sector = sector_mapping[sector_mapping['Ticker'] == ticker]['Sector'].iloc[0]\n",
        "            print(f\"- {ticker} ({sector})\")\n",
        "\n",
        "# Get sectors and verify coverage\n",
        "    sectors = df['Sector'].unique()\n",
        "    sector_coverage = df.groupby('Sector')['Ticker'].nunique()\n",
        "    total_sectors = sector_mapping['Sector'].nunique()\n",
        "    \n",
        "    if len(sectors) < total_sectors:\n",
        "        missing_sectors = set(sector_mapping['Sector'].unique()) - set(sectors)\n",
        "        print(\"\\nWarning: Missing sectors:\")\n",
        "        for sector in missing_sectors:\n",
        "            print(f\"- {sector}\")\n",
        "    \n",
        "    # Verify feature availability\n",
        "    print(\"\\nFeature Availability:\")\n",
        "    for group_name, group_info in FEATURE_GROUPS.items():\n",
        "        available = [f for f in group_info['features'] if f in df.columns]\n",
        "        missing = [f for f in group_info['features'] if f not in df.columns]\n",
        "        \n",
        "        print(f\"\\n{group_name} Features:\")\n",
        "        print(f\"- Available ({len(available)}/{len(group_info['features'])}):\")\n",
        "        for feat in available:\n",
        "            print(f\"  * {feat}\")\n",
        "        \n",
        "        if missing:\n",
        "            print(f\"- Missing ({len(missing)}):\")\n",
        "            for feat in missing:\n",
        "                print(f\"  * {feat}\")\n",
        "    \n",
        "    # Data quality validation\n",
        "    print(\"\\nData Quality Validation:\")\n",
        "    quality_stats = {\n",
        "        'total_stocks': len(available_tickers),\n",
        "        'total_sectors': len(sectors),\n",
        "        'avg_days_per_stock': df.groupby('Ticker').size().mean(),\n",
        "        'min_days_per_stock': df.groupby('Ticker').size().min(),\n",
        "        'missing_values_pct': (df[FEATURE_COLUMNS].isnull().sum() / len(df) * 100).mean(),\n",
        "        'stocks_with_issues': []\n",
        "    }\n",
        "    \n",
        "    # Validate each stock\n",
        "    for ticker in tqdm(available_tickers, desc=\"Validating stocks\"):\n",
        "        stock_data = df[df['Ticker'] == ticker]\n",
        "        issues = []\n",
        "        \n",
        "        # Check data volume\n",
        "        if len(stock_data) < FEATURE_CONFIG['min_samples']:\n",
        "            issues.append(f\"Insufficient samples: {len(stock_data)}\")\n",
        "        \n",
        "        # Check class balance\n",
        "        if 'Label' in stock_data.columns:\n",
        "            label_dist = stock_data['Label'].value_counts(normalize=True) * 100\n",
        "            if label_dist.min() < FEATURE_CONFIG['min_class_pct']:\n",
        "                issues.append(f\"Class imbalance: min class = {label_dist.min():.1f}%\")\n",
        "        \n",
        "        # Check missing values\n",
        "        missing_pct = stock_data[FEATURE_COLUMNS].isnull().mean() * 100\n",
        "        if missing_pct.max() > 0:\n",
        "            issues.append(f\"Missing values: max {missing_pct.max():.1f}%\")\n",
        "        \n",
        "        if issues:\n",
        "            quality_stats['stocks_with_issues'].append((ticker, issues))\n",
        "    \n",
        "    # Print quality summary\n",
        "    print(\"\\nQuality Summary:\")\n",
        "    print(f\"- Stocks: {quality_stats['total_stocks']} across {quality_stats['total_sectors']} sectors\")\n",
        "    print(f\"- Average days per stock: {quality_stats['avg_days_per_stock']:.1f}\")\n",
        "    print(f\"- Minimum days per stock: {quality_stats['min_days_per_stock']}\")\n",
        "    print(f\"- Average missing values: {quality_stats['missing_values_pct']:.2f}%\")\n",
        "    \n",
        "    if quality_stats['stocks_with_issues']:\n",
        "        print(\"\\nStocks with Quality Issues:\")\n",
        "        for ticker, issues in quality_stats['stocks_with_issues']:\n",
        "            print(f\"- {ticker}:\")\n",
        "            for issue in issues:\n",
        "                print(f\"  * {issue}\")\n",
        "    \n",
        "    # Save quality report\n",
        "    quality_report_path = f'../reports/data_quality_{timestamp}.txt'\n",
        "    with open(quality_report_path, 'w') as f:\n",
        "        f.write(\"NASDAQ-100 Feature Selection Data Quality Report\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(\"Dataset Overview:\\n\")\n",
        "        f.write(f\"- Total records: {len(df):,}\\n\")\n",
        "        f.write(f\"- Date range: {df['Date'].min():%Y-%m-%d} to {df['Date'].max():%Y-%m-%d}\\n\")\n",
        "        f.write(f\"- Stocks: {quality_stats['total_stocks']}\\n\")\n",
        "        f.write(f\"- Sectors: {quality_stats['total_sectors']}\\n\\n\")\n",
        "        \n",
        "        f.write(\"Data Quality Metrics:\\n\")\n",
        "        f.write(f\"- Average days per stock: {quality_stats['avg_days_per_stock']:.1f}\\n\")\n",
        "        f.write(f\"- Minimum days per stock: {quality_stats['min_days_per_stock']}\\n\")\n",
        "        f.write(f\"- Average missing values: {quality_stats['missing_values_pct']:.2f}%\\n\\n\")\n",
        "        \n",
        "        if quality_stats['stocks_with_issues']:\n",
        "            f.write(\"Stocks with Quality Issues:\\n\")\n",
        "            for ticker, issues in quality_stats['stocks_with_issues']:\n",
        "                f.write(f\"\\n{ticker}:\\n\")\n",
        "                for issue in issues:\n",
        "                    f.write(f\"- {issue}\\n\")\n",
        "    \n",
        "    print(f\"\\nDetailed quality report saved to: {quality_report_path}\")\n",
        "    print(f\"\\nReady to process {len(available_tickers)} stocks across {len(sectors)} sectors\")\n",
        "    \n",
        "    # Set up for processing\n",
        "    tickers = [t for t in available_tickers if t not in [i[0] for i in quality_stats['stocks_with_issues']]]\n",
        "    ticker_sectors = df.groupby('Ticker')['Sector'].first()\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    logging.error(\"Required dataset not found. Please run feature engineering notebook first.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading data: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculating feature importance by sector...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:   0%|          | 0/11 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Information Technology sector (51 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Information Technology stocks: 100%|██████████| 51/51 [00:42<00:00,  1.21it/s]\n",
            "Processing sectors:   9%|▉         | 1/11 [00:42<07:01, 42.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Information Technology Sector Summary:\n",
            "- Processed 51 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- OBV (Volume): 0.0182 ± 0.0273\n",
            "- High (Price): 0.0182 ± 0.0278\n",
            "- Log_Return (Volatility): 0.0172 ± 0.0265\n",
            "- Daily_Return (Volatility): 0.0171 ± 0.0262\n",
            "- Close (Price): 0.0170 ± 0.0293\n",
            "- Open (Price): 0.0170 ± 0.0288\n",
            "- EMA_10 (Technical): 0.0167 ± 0.0297\n",
            "- Low (Price): 0.0149 ± 0.0286\n",
            "- RSI (Technical): 0.0148 ± 0.0251\n",
            "- BB_Width (Technical): 0.0146 ± 0.0272\n",
            "\n",
            "Processing Consumer Discretionary sector (14 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Consumer Discretionary stocks: 100%|██████████| 14/14 [00:10<00:00,  1.37it/s]\n",
            "Processing sectors:  18%|█▊        | 2/11 [00:52<03:30, 23.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Consumer Discretionary Sector Summary:\n",
            "- Processed 14 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Log_Return (Volatility): 0.0158 ± 0.0253\n",
            "- Daily_Return (Volatility): 0.0155 ± 0.0251\n",
            "- Open (Price): 0.0141 ± 0.0214\n",
            "- OBV (Volume): 0.0128 ± 0.0187\n",
            "- High (Price): 0.0119 ± 0.0178\n",
            "- Close (Price): 0.0117 ± 0.0193\n",
            "- BB_Lower (Technical): 0.0114 ± 0.0240\n",
            "- EMA_10 (Technical): 0.0114 ± 0.0215\n",
            "- BB_Width (Technical): 0.0109 ± 0.0190\n",
            "- Volatility (Volatility): 0.0109 ± 0.0196\n",
            "\n",
            "Processing Utilities sector (3 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Utilities stocks: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
            "Processing sectors:  27%|██▋       | 3/11 [00:54<01:50, 13.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Utilities Sector Summary:\n",
            "- Processed 3 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Close (Price): 0.0081 ± 0.0140\n",
            "- Low (Price): 0.0056 ± 0.0082\n",
            "- Open (Price): 0.0053 ± 0.0089\n",
            "- Sector_Return (Sector): 0.0053 ± 0.0071\n",
            "- MACD_Signal (Technical): 0.0046 ± 0.0079\n",
            "- RS_MA (Sector): 0.0035 ± 0.0017\n",
            "- MACD (Technical): 0.0031 ± 0.0036\n",
            "- High (Price): 0.0031 ± 0.0038\n",
            "- Volatility (Volatility): 0.0031 ± 0.0029\n",
            "- EMA_10 (Technical): 0.0030 ± 0.0029\n",
            "\n",
            "Processing Healthcare sector (11 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Healthcare stocks: 100%|██████████| 11/11 [00:09<00:00,  1.11it/s]\n",
            "Processing sectors:  36%|███▋      | 4/11 [01:04<01:25, 12.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Healthcare Sector Summary:\n",
            "- Processed 11 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- OBV (Volume): 0.0180 ± 0.0211\n",
            "- Open (Price): 0.0168 ± 0.0170\n",
            "- Low (Price): 0.0151 ± 0.0177\n",
            "- Log_Return (Volatility): 0.0147 ± 0.0183\n",
            "- Daily_Return (Volatility): 0.0146 ± 0.0179\n",
            "- BB_Lower (Technical): 0.0146 ± 0.0201\n",
            "- Close (Price): 0.0129 ± 0.0187\n",
            "- EMA_10 (Technical): 0.0126 ± 0.0158\n",
            "- High (Price): 0.0120 ± 0.0130\n",
            "- BB_Width (Technical): 0.0119 ± 0.0133\n",
            "\n",
            "Processing Materials sector (3 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Materials stocks: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]\n",
            "Processing sectors:  45%|████▌     | 5/11 [01:08<00:55,  9.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Materials Sector Summary:\n",
            "- Processed 3 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- RSI (Technical): 0.0125 ± 0.0100\n",
            "- MACD (Technical): 0.0102 ± 0.0098\n",
            "- Volume (Price): 0.0089 ± 0.0054\n",
            "- RS_MA (Sector): 0.0083 ± 0.0033\n",
            "- Volume_MA (Volume): 0.0079 ± 0.0055\n",
            "- Open (Price): 0.0059 ± 0.0049\n",
            "- Close (Price): 0.0058 ± 0.0070\n",
            "- MACD_Hist (Technical): 0.0052 ± 0.0080\n",
            "- MACD_Signal (Technical): 0.0050 ± 0.0082\n",
            "- Log_Return (Volatility): 0.0039 ± 0.0067\n",
            "\n",
            "Processing Energy sector (2 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Energy stocks: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n",
            "Processing sectors:  55%|█████▍    | 6/11 [01:10<00:33,  6.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Energy Sector Summary:\n",
            "- Processed 2 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- BB_Lower (Technical): 0.0079 ± 0.0027\n",
            "- High (Price): 0.0078 ± 0.0011\n",
            "- BB_Width (Technical): 0.0073 ± 0.0020\n",
            "- Volume (Price): 0.0053 ± 0.0076\n",
            "- MACD_Hist (Technical): 0.0050 ± 0.0025\n",
            "- Open (Price): 0.0046 ± 0.0064\n",
            "- Volume_Ratio (Volume): 0.0045 ± 0.0064\n",
            "- Sector_Return (Sector): 0.0039 ± 0.0055\n",
            "- EMA_50 (Technical): 0.0034 ± 0.0047\n",
            "- Volume_MA (Volume): 0.0016 ± 0.0022\n",
            "\n",
            "Processing Communication Services sector (6 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Communication Services stocks: 100%|██████████| 6/6 [00:04<00:00,  1.23it/s]\n",
            "Processing sectors:  64%|██████▎   | 7/11 [01:15<00:24,  6.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Communication Services Sector Summary:\n",
            "- Processed 6 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Daily_Return (Volatility): 0.0186 ± 0.0090\n",
            "- Log_Return (Volatility): 0.0184 ± 0.0091\n",
            "- Low (Price): 0.0151 ± 0.0105\n",
            "- MACD_Signal (Technical): 0.0095 ± 0.0059\n",
            "- Close (Price): 0.0094 ± 0.0147\n",
            "- OBV (Volume): 0.0093 ± 0.0110\n",
            "- Open (Price): 0.0091 ± 0.0068\n",
            "- MACD (Technical): 0.0090 ± 0.0120\n",
            "- High (Price): 0.0074 ± 0.0045\n",
            "- EMA_10 (Technical): 0.0071 ± 0.0082\n",
            "\n",
            "Processing Financials sector (2 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Financials stocks: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n",
            "Processing sectors:  73%|███████▎  | 8/11 [01:17<00:14,  4.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Financials Sector Summary:\n",
            "- Processed 2 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- RSI (Technical): 0.0567 ± 0.0496\n",
            "- Close (Price): 0.0545 ± 0.0511\n",
            "- EMA_50 (Technical): 0.0538 ± 0.0512\n",
            "- MACD_Signal (Technical): 0.0538 ± 0.0264\n",
            "- BB_Upper (Technical): 0.0534 ± 0.0756\n",
            "- High (Price): 0.0508 ± 0.0584\n",
            "- Low (Price): 0.0496 ± 0.0701\n",
            "- Volatility (Volatility): 0.0489 ± 0.0692\n",
            "- Open (Price): 0.0473 ± 0.0669\n",
            "- BB_Lower (Technical): 0.0462 ± 0.0615\n",
            "\n",
            "Processing Consumer Staples sector (6 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Consumer Staples stocks: 100%|██████████| 6/6 [00:05<00:00,  1.09it/s]\n",
            "Processing sectors:  82%|████████▏ | 9/11 [01:22<00:09,  4.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Consumer Staples Sector Summary:\n",
            "- Processed 6 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Log_Return (Volatility): 0.0085 ± 0.0047\n",
            "- Daily_Return (Volatility): 0.0084 ± 0.0049\n",
            "- BB_Width (Technical): 0.0080 ± 0.0057\n",
            "- Sector_Return (Sector): 0.0079 ± 0.0093\n",
            "- BB_Upper (Technical): 0.0074 ± 0.0130\n",
            "- BB_Lower (Technical): 0.0060 ± 0.0053\n",
            "- High (Price): 0.0046 ± 0.0051\n",
            "- Close (Price): 0.0043 ± 0.0056\n",
            "- Volume_MA (Volume): 0.0042 ± 0.0071\n",
            "- MACD_Hist (Technical): 0.0030 ± 0.0056\n",
            "\n",
            "Processing Industrials sector (7 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Industrials stocks: 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n",
            "Processing sectors:  91%|█████████ | 10/11 [01:28<00:05,  5.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Industrials Sector Summary:\n",
            "- Processed 7 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Close (Price): 0.0133 ± 0.0176\n",
            "- RSI (Technical): 0.0130 ± 0.0219\n",
            "- High (Price): 0.0113 ± 0.0078\n",
            "- OBV (Volume): 0.0102 ± 0.0103\n",
            "- EMA_10 (Technical): 0.0085 ± 0.0160\n",
            "- BB_Width (Technical): 0.0084 ± 0.0135\n",
            "- Daily_Return (Volatility): 0.0074 ± 0.0114\n",
            "- Log_Return (Volatility): 0.0071 ± 0.0106\n",
            "- Relative_Strength (Sector): 0.0061 ± 0.0065\n",
            "- BB_Upper (Technical): 0.0061 ± 0.0083\n",
            "\n",
            "Processing Real Estate sector (3 stocks)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Real Estate stocks: 100%|██████████| 3/3 [00:02<00:00,  1.10it/s]\n",
            "Processing sectors: 100%|██████████| 11/11 [01:31<00:00,  8.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Real Estate Sector Summary:\n",
            "- Processed 3 stocks successfully\n",
            "\n",
            "Top 10 Features:\n",
            "- Open (Price): 0.0085 ± 0.0064\n",
            "- Sector_Return (Sector): 0.0075 ± 0.0077\n",
            "- Volume_MA (Volume): 0.0067 ± 0.0059\n",
            "- Low (Price): 0.0060 ± 0.0103\n",
            "- Volume (Price): 0.0058 ± 0.0050\n",
            "- MACD (Technical): 0.0055 ± 0.0064\n",
            "- RS_MA (Sector): 0.0048 ± 0.0083\n",
            "- Volatility (Volatility): 0.0044 ± 0.0048\n",
            "- BB_Upper (Technical): 0.0042 ± 0.0052\n",
            "- BB_Lower (Technical): 0.0039 ± 0.0050\n",
            "\n",
            "Saving final results...\n",
            "Summary report saved to: ../reports/feature_selection_summary_20250727_223141.txt\n",
            "\n",
            "Feature selection analysis complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Enhanced SMI calculation with sector context\n",
        "def calculate_feature_importance(stock_data: pd.DataFrame, features: list, config: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate feature importance scores with sector context.\n",
        "    \n",
        "    Args:\n",
        "        stock_data (pd.DataFrame): Stock data with features and labels\n",
        "        features (list): List of features to analyze\n",
        "        config (dict): Configuration parameters\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (feature scores, feature ranks, temporal stability)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verify data requirements\n",
        "        if len(stock_data) < config['min_samples']:\n",
        "            raise ValueError(f\"Insufficient samples: {len(stock_data)}\")\n",
        "        \n",
        "        # Create label (1 if next day's return is positive, 0 otherwise)\n",
        "        stock_data['Label'] = (stock_data['Close'].shift(-1) / stock_data['Close'] - 1 > 0).astype(int)\n",
        "        stock_data = stock_data.iloc[:-1]  # Remove last row since we can't calculate next day's return\n",
        "        \n",
        "        # Prepare features and labels\n",
        "        X = stock_data[features].copy()\n",
        "        y = stock_data['Label']\n",
        "        \n",
        "        # Handle missing and infinite values\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "        X = X.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = pd.DataFrame(\n",
        "            scaler.fit_transform(X),\n",
        "            columns=features,\n",
        "            index=X.index\n",
        "        )\n",
        "        \n",
        "        # Calculate base SMI scores\n",
        "        base_scores = pd.Series(\n",
        "            mutual_info_classif(X_scaled, y, random_state=config['smi_random_state']),\n",
        "            index=features\n",
        "        )\n",
        "        \n",
        "        # Calculate temporal stability\n",
        "        stability_scores = {}\n",
        "        window_size = len(stock_data) // 5  # Use 5 windows\n",
        "        \n",
        "        for i in range(5):\n",
        "            start_idx = i * window_size\n",
        "            end_idx = start_idx + window_size\n",
        "            window_data = X_scaled.iloc[start_idx:end_idx]\n",
        "            window_labels = y.iloc[start_idx:end_idx]\n",
        "            \n",
        "            window_scores = pd.Series(\n",
        "                mutual_info_classif(window_data, window_labels, \n",
        "                                  random_state=config['smi_random_state']),\n",
        "                index=features\n",
        "            )\n",
        "            stability_scores[f'window_{i}'] = window_scores\n",
        "        \n",
        "        stability_df = pd.DataFrame(stability_scores)\n",
        "        temporal_stability = 1 - stability_df.std(axis=1) / stability_df.mean(axis=1)\n",
        "        \n",
        "        # Calculate feature ranks\n",
        "        ranks = base_scores.rank(ascending=False)\n",
        "        \n",
        "        return base_scores, ranks, temporal_stability\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in feature importance calculation: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Process stocks by sector\n",
        "print(\"\\nCalculating feature importance by sector...\")\n",
        "results = {\n",
        "    'stocks': {},      # Stock-level results\n",
        "    'sectors': {},     # Sector-level results\n",
        "    'features': {},    # Feature-level statistics\n",
        "    'correlations': {} # Feature correlations\n",
        "}\n",
        "\n",
        "# Process each sector\n",
        "for sector in tqdm(sectors, desc=\"Processing sectors\"):\n",
        "    try:\n",
        "        sector_tickers = df[df['Sector'] == sector]['Ticker'].unique()\n",
        "        print(f\"\\nProcessing {sector} sector ({len(sector_tickers)} stocks)...\")\n",
        "        \n",
        "        sector_scores = []\n",
        "        sector_stability = []\n",
        "        \n",
        "        # Process each stock in sector\n",
        "        for ticker in tqdm(sector_tickers, desc=f\"Processing {sector} stocks\"):\n",
        "            try:\n",
        "                # Get stock data\n",
        "                stock_data = df[df['Ticker'] == ticker].copy()\n",
        "                \n",
        "                # Calculate feature importance\n",
        "                scores, ranks, stability = calculate_feature_importance(\n",
        "                    stock_data, FEATURE_COLUMNS, FEATURE_CONFIG\n",
        "                )\n",
        "                \n",
        "                if scores is not None:\n",
        "                    # Store stock results\n",
        "                    results['stocks'][ticker] = {\n",
        "                        'ticker': ticker,\n",
        "                        'sector': sector,\n",
        "                        'scores': scores,\n",
        "                        'ranks': ranks,\n",
        "                        'stability': stability,\n",
        "                        'top_features': scores.nlargest(10).index.tolist(),\n",
        "                        'top_scores': scores.nlargest(10).values.tolist()\n",
        "                    }\n",
        "                    \n",
        "                    # Add to sector aggregates\n",
        "                    sector_scores.append(scores)\n",
        "                    sector_stability.append(stability)\n",
        "                    \n",
        "                    # Save stock-level results\n",
        "                    stock_df = pd.DataFrame({\n",
        "                        'Feature': scores.index,\n",
        "                        'MI_Score': scores.values,\n",
        "                        'Rank': ranks.values,\n",
        "                        'Stability': stability.values,\n",
        "                        'Feature_Group': [FEATURE_DESCRIPTIONS[f]['group'] for f in scores.index],\n",
        "                        'Description': [FEATURE_DESCRIPTIONS[f]['description'] for f in scores.index]\n",
        "                    })\n",
        "                    stock_df = stock_df.sort_values('MI_Score', ascending=False)\n",
        "                    stock_df.to_csv(f'../data/smi_scores/{ticker}_features.csv', index=False)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        # Calculate sector-level statistics\n",
        "        if sector_scores:\n",
        "            sector_scores_df = pd.DataFrame(sector_scores)\n",
        "            sector_stability_df = pd.DataFrame(sector_stability)\n",
        "            \n",
        "            results['sectors'][sector] = {\n",
        "                'mean_scores': sector_scores_df.mean(),\n",
        "                'std_scores': sector_scores_df.std(),\n",
        "                'mean_stability': sector_stability_df.mean(),\n",
        "                'feature_ranks': sector_scores_df.mean().rank(ascending=False),\n",
        "                'top_features': sector_scores_df.mean().nlargest(10).index.tolist(),\n",
        "                'stock_count': len(sector_scores)\n",
        "            }\n",
        "            \n",
        "            # Save sector-level results\n",
        "            sector_df = pd.DataFrame({\n",
        "                'Feature': results['sectors'][sector]['mean_scores'].index,\n",
        "                'Mean_Score': results['sectors'][sector]['mean_scores'].values,\n",
        "                'Std_Score': results['sectors'][sector]['std_scores'].values,\n",
        "                'Mean_Stability': results['sectors'][sector]['mean_stability'].values,\n",
        "                'Rank': results['sectors'][sector]['feature_ranks'].values,\n",
        "                'Feature_Group': [FEATURE_DESCRIPTIONS[f]['group'] for f in results['sectors'][sector]['mean_scores'].index],\n",
        "                'Description': [FEATURE_DESCRIPTIONS[f]['description'] for f in results['sectors'][sector]['mean_scores'].index]\n",
        "            })\n",
        "            sector_df = sector_df.sort_values('Mean_Score', ascending=False)\n",
        "            sector_df.to_csv(f'../data/smi_scores/{sector}_features.csv', index=False)\n",
        "            \n",
        "            # Print sector summary\n",
        "            print(f\"\\n{sector} Sector Summary:\")\n",
        "            print(f\"- Processed {len(sector_scores)} stocks successfully\")\n",
        "            print(\"\\nTop 10 Features:\")\n",
        "            top_features = sector_df.head(10)\n",
        "            for _, row in top_features.iterrows():\n",
        "                print(f\"- {row['Feature']} ({row['Feature_Group']}): {row['Mean_Score']:.4f} ± {row['Std_Score']:.4f}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing sector {sector}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Calculate cross-sector statistics\n",
        "if results['sectors']:\n",
        "    # Combine sector scores\n",
        "    sector_scores = pd.DataFrame({\n",
        "        sector: data['mean_scores'] \n",
        "        for sector, data in results['sectors'].items()\n",
        "    })\n",
        "    \n",
        "    # Calculate feature-level statistics\n",
        "    for feature in FEATURE_COLUMNS:\n",
        "        results['features'][feature] = {\n",
        "            'global_rank': sector_scores.loc[feature].mean(),\n",
        "            'rank_stability': 1 - sector_scores.loc[feature].std() / sector_scores.loc[feature].mean(),\n",
        "            'top_sectors': sector_scores.loc[feature].nlargest(3).index.tolist(),\n",
        "            'feature_group': FEATURE_DESCRIPTIONS[feature]['group']\n",
        "        }\n",
        "    \n",
        "    # Calculate feature correlations\n",
        "    feature_corr = sector_scores.corr()\n",
        "    results['correlations'] = {\n",
        "        'matrix': feature_corr,\n",
        "        'high_correlations': []\n",
        "    }\n",
        "    \n",
        "    # Find highly correlated features\n",
        "    for i in range(len(feature_corr.columns)):\n",
        "        for j in range(i+1, len(feature_corr.columns)):\n",
        "            corr = feature_corr.iloc[i, j]\n",
        "            if abs(corr) > FEATURE_CONFIG['correlation_threshold']:\n",
        "                results['correlations']['high_correlations'].append({\n",
        "                    'feature1': feature_corr.columns[i],\n",
        "                    'feature2': feature_corr.columns[j],\n",
        "                    'correlation': corr\n",
        "                })\n",
        "\n",
        "    # Save final results\n",
        "print(\"\\nSaving final results...\")\n",
        "\n",
        "# Initialize results structure if not already initialized\n",
        "if 'correlations' not in results:\n",
        "    results['correlations'] = {'high_correlations': []}\n",
        "\n",
        "# 1. Create summary report\n",
        "summary_path = f'../reports/feature_selection_summary_{timestamp}.txt'\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"NASDAQ-100 Feature Selection Summary\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    \n",
        "    # Overall statistics\n",
        "    f.write(\"Analysis Coverage:\\n\")\n",
        "    f.write(f\"- Total stocks analyzed: {len(results['stocks'])}\\n\")\n",
        "    f.write(f\"- Sectors covered: {len(results['sectors'])}\\n\\n\")\n",
        "    \n",
        "    # Sector summaries\n",
        "    f.write(\"Sector-wise Feature Importance:\\n\")\n",
        "    for sector, data in results['sectors'].items():\n",
        "        f.write(f\"\\n{sector}:\\n\")\n",
        "        f.write(f\"- Stocks analyzed: {data['stock_count']}\\n\")\n",
        "        f.write(\"- Top 5 features:\\n\")\n",
        "        for feature in data['top_features'][:5]:\n",
        "            score = data['mean_scores'][feature]\n",
        "            stability = data['mean_stability'][feature]\n",
        "            f.write(f\"  * {feature}: score={score:.4f}, stability={stability:.4f}\\n\")\n",
        "    \n",
        "    # Feature correlations\n",
        "    if results['correlations']['high_correlations']:\n",
        "        f.write(\"\\nHighly Correlated Features:\\n\")\n",
        "        for corr in results['correlations']['high_correlations']:\n",
        "            f.write(f\"- {corr['feature1']} ↔ {corr['feature2']}: {corr['correlation']:.4f}\\n\")\n",
        "    \n",
        "    # Global feature ranking\n",
        "    if 'features' in results:\n",
        "        f.write(\"\\nGlobal Feature Ranking:\\n\")\n",
        "        global_ranks = pd.DataFrame(results['features']).T\n",
        "        top_features = global_ranks.sort_values('global_rank', ascending=False).head(10)\n",
        "        for feature, data in top_features.iterrows():\n",
        "            f.write(f\"- {feature} ({data['feature_group']}):\\n\")\n",
        "            f.write(f\"  * Global rank: {data['global_rank']:.4f}\\n\")\n",
        "            f.write(f\"  * Rank stability: {data['rank_stability']:.4f}\\n\")\n",
        "            f.write(f\"  * Best sectors: {', '.join(data['top_sectors'])}\\n\")\n",
        "\n",
        "print(f\"Summary report saved to: {summary_path}\")\n",
        "print(\"\\nFeature selection analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating feature importance visualizations...\n",
            "\n",
            "Visualization files saved:\n",
            "- Feature importance: ../reports/figures/features/\n",
            "- Sector analysis: ../reports/figures/sectors/\n",
            "- Summary report: ../reports/feature_selection_summary_20250727_223141.txt\n",
            "\n",
            "Feature selection analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Create visualizations\n",
        "print(\"\\nGenerating feature importance visualizations...\")\n",
        "\n",
        "try:\n",
        "    # 1. Sector-wise Feature Importance Heatmap\n",
        "    plt.figure(figsize=(20, 12))\n",
        "    \n",
        "    # Prepare data for heatmap\n",
        "    sector_scores = pd.DataFrame({\n",
        "        sector: data['mean_scores'] \n",
        "        for sector, data in results['sectors'].items()\n",
        "    })\n",
        "    \n",
        "    # Sort features by global importance\n",
        "    global_importance = sector_scores.mean(axis=1)\n",
        "    sector_scores = sector_scores.loc[global_importance.sort_values(ascending=False).index]\n",
        "    \n",
        "    # Create heatmap\n",
        "    sns.heatmap(sector_scores, cmap='YlOrRd', annot=True, fmt='.3f',\n",
        "                xticklabels=True, yticklabels=True, center=0)\n",
        "    plt.title('Feature Importance by Sector', pad=20)\n",
        "    plt.xlabel('Sector', labelpad=10)\n",
        "    plt.ylabel('Feature', labelpad=10)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    \n",
        "    # Add feature group annotations\n",
        "    feature_groups = [FEATURE_DESCRIPTIONS[f]['group'] for f in sector_scores.index]\n",
        "    plt.gca().set_yticklabels([f\"{feat} ({group})\" for feat, group in zip(sector_scores.index, feature_groups)])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../reports/figures/features/importance_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # 2. Feature Group Performance by Sector\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Calculate group-wise statistics\n",
        "    group_stats = []\n",
        "    for sector, data in results['sectors'].items():\n",
        "        scores = data['mean_scores']\n",
        "        stability = data['mean_stability']\n",
        "        \n",
        "        for group_name, group_info in FEATURE_GROUPS.items():\n",
        "            group_features = [f for f in group_info['features'] if f in scores.index]\n",
        "            if group_features:\n",
        "                group_stats.append({\n",
        "                    'Sector': sector,\n",
        "                    'Feature_Group': group_name,\n",
        "                    'Mean_Score': scores[group_features].mean(),\n",
        "                    'Mean_Stability': stability[group_features].mean(),\n",
        "                    'Feature_Count': len(group_features)\n",
        "                })\n",
        "    \n",
        "    group_df = pd.DataFrame(group_stats)\n",
        "    \n",
        "    # Create grouped violin plot\n",
        "    sns.violinplot(data=group_df, x='Feature_Group', y='Mean_Score',\n",
        "                  hue='Sector', palette=SECTOR_COLORS, split=True)\n",
        "    \n",
        "    plt.title('Feature Group Performance Distribution', pad=20)\n",
        "    plt.xlabel('Feature Group', labelpad=10)\n",
        "    plt.ylabel('Mean Importance Score', labelpad=10)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend(title='Sector', bbox_to_anchor=(1.05, 1))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../reports/figures/features/group_performance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # 3. Feature Stability Analysis\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Calculate stability statistics\n",
        "    stability_stats = pd.DataFrame({\n",
        "        sector: data['mean_stability']\n",
        "        for sector, data in results['sectors'].items()\n",
        "    })\n",
        "    \n",
        "    # Sort by average stability\n",
        "    avg_stability = stability_stats.mean(axis=1)\n",
        "    stability_stats = stability_stats.loc[avg_stability.sort_values(ascending=False).index]\n",
        "    \n",
        "    # Create stability heatmap\n",
        "    sns.heatmap(stability_stats, cmap='viridis', annot=True, fmt='.2f',\n",
        "                xticklabels=True, yticklabels=True)\n",
        "    plt.title('Feature Stability by Sector', pad=20)\n",
        "    plt.xlabel('Sector', labelpad=10)\n",
        "    plt.ylabel('Feature', labelpad=10)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../reports/figures/features/stability_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # 4. Feature Correlation Network (simplified to avoid group attribute issues)\n",
        "    if results['correlations']['high_correlations']:\n",
        "        plt.figure(figsize=(15, 15))\n",
        "        \n",
        "        # Create correlation network\n",
        "        G = nx.Graph()\n",
        "        \n",
        "        # Create mapping of features to their groups\n",
        "        feature_to_group = {}\n",
        "        for group_name, group_info in FEATURE_GROUPS.items():\n",
        "            for feature in group_info['features']:\n",
        "                feature_to_group[feature] = group_name\n",
        "        \n",
        "        # Add nodes colored by feature group\n",
        "        for feature in FEATURE_COLUMNS:\n",
        "            group = feature_to_group.get(feature, 'Unknown')\n",
        "            G.add_node(feature, group=group)\n",
        "        \n",
        "        # Add edges for highly correlated features\n",
        "        for corr in results['correlations']['high_correlations']:\n",
        "            G.add_edge(\n",
        "                corr['feature1'],\n",
        "                corr['feature2'],\n",
        "                weight=abs(corr['correlation'])\n",
        "            )\n",
        "        \n",
        "        # Ensure all nodes have group attribute\n",
        "        for node in G.nodes():\n",
        "            if 'group' not in G.nodes[node]:\n",
        "                G.nodes[node]['group'] = feature_to_group.get(node, 'Unknown')\n",
        "        \n",
        "        # Set node colors by group\n",
        "        node_colors = []\n",
        "        for n in G.nodes():\n",
        "            group = G.nodes[n]['group']\n",
        "            if group in list(FEATURE_GROUPS.keys()):\n",
        "                color_idx = list(FEATURE_GROUPS.keys()).index(group) / len(FEATURE_GROUPS)\n",
        "                node_colors.append(plt.cm.tab20(color_idx))\n",
        "            else:\n",
        "                node_colors.append(plt.cm.tab20(0))  # Default color\n",
        "        \n",
        "        # Draw network\n",
        "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
        "        \n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(G, pos,\n",
        "                              node_color=node_colors,\n",
        "                              node_size=1000,\n",
        "                              alpha=0.7)\n",
        "        \n",
        "        # Draw edges with varying thickness\n",
        "        edge_weights = [G[u][v]['weight'] * 2 for u, v in G.edges()]\n",
        "        nx.draw_networkx_edges(G, pos,\n",
        "                              width=edge_weights,\n",
        "                              alpha=0.5)\n",
        "        \n",
        "        # Add labels\n",
        "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "        \n",
        "        # Add legend\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w',\n",
        "                      markerfacecolor=plt.cm.tab20(i/len(FEATURE_GROUPS)),\n",
        "                      label=group, markersize=10)\n",
        "            for i, group in enumerate(FEATURE_GROUPS.keys())\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements,\n",
        "                  title='Feature Groups',\n",
        "                  loc='center left',\n",
        "                  bbox_to_anchor=(1, 0.5))\n",
        "        \n",
        "        plt.title('Feature Correlation Network\\n(edges: |correlation| > 0.7)', pad=20)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../reports/figures/features/correlation_network.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    \n",
        "    # 5. Sector-wise Feature Distribution\n",
        "    for sector in results['sectors'].keys():\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        \n",
        "        # Get sector data\n",
        "        sector_data = pd.DataFrame({\n",
        "            'Score': results['sectors'][sector]['mean_scores'],\n",
        "            'Stability': results['sectors'][sector]['mean_stability'],\n",
        "            'Group': [FEATURE_DESCRIPTIONS[f]['group'] for f in results['sectors'][sector]['mean_scores'].index]\n",
        "        })\n",
        "        \n",
        "        # Create scatter plot\n",
        "        sns.scatterplot(data=sector_data,\n",
        "                       x='Score', y='Stability',\n",
        "                       hue='Group', size='Score',\n",
        "                       sizes=(50, 400), alpha=0.6)\n",
        "        \n",
        "        plt.title(f'{sector} Sector - Feature Importance vs Stability', pad=20)\n",
        "        plt.xlabel('Importance Score', labelpad=10)\n",
        "        plt.ylabel('Temporal Stability', labelpad=10)\n",
        "        \n",
        "        # Add annotations for top features\n",
        "        top_n = 5\n",
        "        top_features = sector_data.nlargest(top_n, 'Score').index\n",
        "        \n",
        "        for feature in top_features:\n",
        "            plt.annotate(\n",
        "                feature,\n",
        "                (sector_data.loc[feature, 'Score'],\n",
        "                 sector_data.loc[feature, 'Stability']),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=8, alpha=0.8\n",
        "            )\n",
        "        \n",
        "        plt.legend(title='Feature Group', bbox_to_anchor=(1.05, 1))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'../reports/figures/sectors/{sector}_features.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error creating visualizations: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\nVisualization files saved:\")\n",
        "print(\"- Feature importance: ../reports/figures/features/\")\n",
        "print(\"- Sector analysis: ../reports/figures/sectors/\")\n",
        "print(f\"- Summary report: {summary_path}\")\n",
        "print(\"\\nFeature selection analysis complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating NASDAQ-100 dataset quality...\n",
            "\n",
            "Expected tickers from sector_mapping.csv: 109\n",
            "Dataset loaded: 3,756 rows, 22 columns\n",
            "\n",
            "⚠️ Missing columns:\n",
            "- Open_Return\n",
            "- High_Return\n",
            "- Low_Return\n",
            "- Close_Return\n",
            "\n",
            "Analyzing 3 available tickers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating stocks:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "❌ Error during validation: \"['Open_Return', 'High_Return', 'Low_Return', 'Close_Return'] not in index\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Quality Validation Script\n",
        "print(\"Validating NASDAQ-100 dataset quality...\")\n",
        "\n",
        "try:\n",
        "    # Load sector mapping for reference\n",
        "    sector_mapping = pd.read_csv('../data/sector_mapping.csv')\n",
        "    expected_tickers = set(sector_mapping['Ticker'].unique())\n",
        "    print(f\"\\nExpected tickers from sector_mapping.csv: {len(expected_tickers)}\")\n",
        "    \n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('../data/labeled_signals_nasdaq.csv')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "    \n",
        "    # Initialize validation results\n",
        "    validation_results = []\n",
        "    \n",
        "    # Required columns by category\n",
        "    REQUIRED_COLUMNS = {\n",
        "        'Base': ['Date', 'Ticker', 'Sector', 'Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "        'Technical': ['EMA10', 'EMA50', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist'],\n",
        "        'Derived': ['volatility', 'volume_ma'],\n",
        "        'Returns': ['Open_Return', 'High_Return', 'Low_Return', 'Close_Return'],\n",
        "        'Labels': ['Label', 'Signal', 'future_return']\n",
        "    }\n",
        "    \n",
        "    # Check for missing columns\n",
        "    all_required = [col for cols in REQUIRED_COLUMNS.values() for col in cols]\n",
        "    missing_cols = [col for col in all_required if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(\"\\n⚠️ Missing columns:\")\n",
        "        for col in missing_cols:\n",
        "            print(f\"- {col}\")\n",
        "    \n",
        "    # Analyze each ticker\n",
        "    available_tickers = df['Ticker'].unique()\n",
        "    missing_tickers = expected_tickers - set(available_tickers)\n",
        "    \n",
        "    print(f\"\\nAnalyzing {len(available_tickers)} available tickers...\")\n",
        "    \n",
        "    for ticker in tqdm(available_tickers, desc=\"Validating stocks\"):\n",
        "        ticker_data = df[df['Ticker'] == ticker]\n",
        "        sector = ticker_data['Sector'].iloc[0]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        total_rows = len(ticker_data)\n",
        "        date_range = (ticker_data['Date'].max() - ticker_data['Date'].min()).days\n",
        "        missing_values = ticker_data[all_required].isnull().sum()\n",
        "        has_missing = missing_values.sum() > 0\n",
        "        \n",
        "        # Check label distribution\n",
        "        label_dist = ticker_data['Label'].value_counts()\n",
        "        signal_dist = ticker_data['Signal'].value_counts()\n",
        "        \n",
        "        validation_results.append({\n",
        "            'Ticker': ticker,\n",
        "            'Sector': sector,\n",
        "            'Total_Rows': total_rows,\n",
        "            'Date_Range_Days': date_range,\n",
        "            'Trading_Days_Per_Year': (total_rows / (date_range/365)) if date_range > 0 else 0,\n",
        "            'Missing_Values': missing_values.sum(),\n",
        "            'Missing_Columns': ', '.join(missing_values[missing_values > 0].index),\n",
        "            'Unique_Labels': len(label_dist),\n",
        "            'Label_Distribution': str(label_dist.to_dict()),\n",
        "            'Signal_Distribution': str(signal_dist.to_dict()),\n",
        "            'Mean_Forward_Return': ticker_data['future_return'].mean(),\n",
        "            'Std_Forward_Return': ticker_data['future_return'].std(),\n",
        "            'Status': 'Low Volume' if total_rows < 2000 else 'OK'\n",
        "        })\n",
        "    \n",
        "    # Create validation DataFrame\n",
        "    validation_df = pd.DataFrame(validation_results)\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(\"\\n=== Dataset Quality Summary ===\")\n",
        "    print(f\"\\nTicker Coverage:\")\n",
        "    print(f\"- Expected tickers: {len(expected_tickers)}\")\n",
        "    print(f\"- Available tickers: {len(available_tickers)}\")\n",
        "    print(f\"- Missing tickers: {len(missing_tickers)}\")\n",
        "    \n",
        "    print(\"\\nData Volume:\")\n",
        "    low_volume = validation_df[validation_df['Total_Rows'] < 2000]\n",
        "    print(f\"- Stocks with <2000 rows: {len(low_volume)}\")\n",
        "    if not low_volume.empty:\n",
        "        print(\"\\nLow volume tickers:\")\n",
        "        for _, row in low_volume.iterrows():\n",
        "            print(f\"- {row['Ticker']} ({row['Sector']}): {row['Total_Rows']} rows\")\n",
        "    \n",
        "    print(\"\\nMissing Values:\")\n",
        "    has_missing = validation_df[validation_df['Missing_Values'] > 0]\n",
        "    if not has_missing.empty:\n",
        "        print(f\"- Stocks with missing values: {len(has_missing)}\")\n",
        "        print(\"\\nStocks with missing data:\")\n",
        "        for _, row in has_missing.iterrows():\n",
        "            print(f\"- {row['Ticker']}: {row['Missing_Values']} missing values in {row['Missing_Columns']}\")\n",
        "    else:\n",
        "        print(\"✓ No missing values found\")\n",
        "    \n",
        "    print(\"\\nLabel Quality:\")\n",
        "    bad_labels = validation_df[validation_df['Unique_Labels'] < 2]\n",
        "    if not bad_labels.empty:\n",
        "        print(f\"- Stocks with insufficient labels: {len(bad_labels)}\")\n",
        "        print(\"\\nStocks with label issues:\")\n",
        "        for _, row in bad_labels.iterrows():\n",
        "            print(f\"- {row['Ticker']}: only {row['Unique_Labels']} unique labels\")\n",
        "    else:\n",
        "        print(\"✓ All stocks have sufficient label variety\")\n",
        "    \n",
        "    # Save detailed report\n",
        "    validation_df.to_csv('../reports/data_quality_report.csv', index=False)\n",
        "    print(\"\\nDetailed quality report saved to: ../reports/data_quality_report.csv\")\n",
        "    \n",
        "    # Print missing tickers by sector\n",
        "    if missing_tickers:\n",
        "        print(\"\\nMissing Tickers by Sector:\")\n",
        "        sector_missing = {}\n",
        "        for ticker in missing_tickers:\n",
        "            sector = sector_mapping[sector_mapping['Ticker'] == ticker]['Sector'].iloc[0]\n",
        "            if sector not in sector_missing:\n",
        "                sector_missing[sector] = []\n",
        "            sector_missing[sector].append(ticker)\n",
        "        \n",
        "        for sector, tickers in sector_missing.items():\n",
        "            print(f\"\\n{sector} ({len(tickers)} missing):\")\n",
        "            for ticker in sorted(tickers):\n",
        "                print(f\"- {ticker}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n❌ Error: Required file not found - {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error during validation: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed feature importance results\n",
        "feature_importance_data = []\n",
        "\n",
        "# Compile per-stock and per-sector statistics\n",
        "for ticker, result in stock_results.items():\n",
        "    sector = result['Sector']\n",
        "    scores = result['Scores']\n",
        "    \n",
        "    # Get sector average scores\n",
        "    sector_avg = sector_importance[sector]\n",
        "    \n",
        "    # Calculate deviation from sector average\n",
        "    score_deviation = scores - sector_avg\n",
        "    \n",
        "    # Add to results\n",
        "    for feature in FEATURE_COLUMNS:\n",
        "        feature_importance_data.append({\n",
        "            'Ticker': ticker,\n",
        "            'Sector': sector,\n",
        "            'Feature': feature,\n",
        "            'Feature_Group': next(group for group, features in FEATURE_GROUPS.items() if feature in features),\n",
        "            'MI_Score': scores[feature],\n",
        "            'Sector_Avg': sector_avg[feature],\n",
        "            'Deviation': score_deviation[feature]\n",
        "        })\n",
        "\n",
        "# Create and save detailed DataFrame\n",
        "detailed_df = pd.DataFrame(feature_importance_data)\n",
        "detailed_df.to_csv('../data/feature_importance_detailed.csv', index=False)\n",
        "\n",
        "# Create feature group summary\n",
        "group_summary = detailed_df.groupby(['Sector', 'Feature_Group'])['MI_Score'].agg(['mean', 'std']).round(4)\n",
        "print(\"\\nFeature Group Importance by Sector:\")\n",
        "print(group_summary)\n",
        "\n",
        "# Save summary statistics\n",
        "group_summary.to_csv('../data/feature_group_summary.csv')\n",
        "\n",
        "print(\"\\nFeature importance analysis files saved:\")\n",
        "print(\"1. feature_importance_smi.csv - Per-stock top features\")\n",
        "print(\"2. feature_importance_detailed.csv - Detailed per-feature scores\")\n",
        "print(\"3. feature_group_summary.csv - Feature group statistics\")\n",
        "print(\"4. sector_feature_importance.png - Heatmap visualization\")\n",
        "print(\"5. sector_top_features.png - Top features by sector\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
