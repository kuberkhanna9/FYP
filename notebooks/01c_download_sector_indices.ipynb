{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NASDAQ-100 Sector ETF Data Processing\n",
        "#\n",
        "# This notebook processes sector ETF data for the NASDAQ-100 stocks.\n",
        "# We use SPDR sector ETFs to track each GICS sector's performance.\n",
        "#\n",
        "# Process:\n",
        "# 1. Load sector ETF data from CSV files\n",
        "# 2. Validate data quality\n",
        "# 3. Save processed data\n",
        "#\n",
        "# Data Specifications:\n",
        "# - Fields: Open, High, Low, Close, Volume\n",
        "# - Output: \n",
        "#   - Individual CSVs in `/data/sectors/{ETF_TICKER}.csv`\n",
        "#   - Merged dataset in `/data/processed/sector_indices_merged.csv`\n",
        "# - Data quality report in `/reports/sector_quality_{timestamp}.log`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pytz\n",
        "\n",
        "# Configure logging\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "quality_report = f'../reports/sector_quality_{timestamp}.log'\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(quality_report)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "START_DATE = '2015-01-01'\n",
        "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
        "UTC = pytz.UTC\n",
        "\n",
        "# Required columns for validation\n",
        "REQUIRED_COLUMNS = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "# Define GICS sectors and their ETFs\n",
        "SECTOR_ETF_MAP = {\n",
        "    'XLK': 'Information Technology',    # Technology sector benchmark\n",
        "    'XLF': 'Financials',               # Banks, Insurance, Investment firms\n",
        "    'XLV': 'Healthcare',               # Healthcare providers, Biotech\n",
        "    'XLE': 'Energy',                   # Oil & Gas companies\n",
        "    'XLY': 'Consumer Discretionary',   # Retail, Automotive, Media\n",
        "    'XLP': 'Consumer Staples',         # Food & Beverage\n",
        "    'XLI': 'Industrials',              # Aerospace & Defense\n",
        "    'XLB': 'Materials',                # Chemicals, Mining\n",
        "    'XLU': 'Utilities',                # Electric & Gas utilities\n",
        "    'XLRE': 'Real Estate',             # REITs & Property management\n",
        "    'XLC': 'Communication Services'     # Telecom services & Media\n",
        "}\n",
        "\n",
        "# Validation configuration\n",
        "VALIDATION_CONFIG = {\n",
        "    'min_trading_days': 1000,  # Minimum number of trading days required\n",
        "    'max_missing_pct': 5,      # Maximum percentage of missing data allowed\n",
        "    'outlier_threshold': 5     # Standard deviations for outlier detection\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions for data validation\n",
        "def validate_data_quality(df, name):\n",
        "    \"\"\"Validate data quality metrics.\"\"\"\n",
        "    try:\n",
        "        # Check date range\n",
        "        if len(df) < VALIDATION_CONFIG['min_trading_days']:\n",
        "            raise ValueError(f\"Insufficient trading days ({len(df)})\")\n",
        "        \n",
        "        # Check missing data\n",
        "        missing_pct = (df[REQUIRED_COLUMNS].isnull().sum() / len(df) * 100).max()\n",
        "        if missing_pct > VALIDATION_CONFIG['max_missing_pct']:\n",
        "            raise ValueError(f\"High missing data ({missing_pct:.1f}%)\")\n",
        "        \n",
        "        # Check for invalid values\n",
        "        for col in ['Open', 'High', 'Low', 'Close']:\n",
        "            if (df[col] <= 0).any():\n",
        "                raise ValueError(f\"Invalid {col} values (<=0)\")\n",
        "            if df[col].isnull().any():\n",
        "                raise ValueError(f\"Missing {col} values\")\n",
        "        \n",
        "        if (df['Volume'] < 0).any():\n",
        "            raise ValueError(f\"Invalid Volume values (<0)\")\n",
        "        \n",
        "        # Check price relationships with tolerance for equal values\n",
        "        invalid_price = (\n",
        "            (df['High'] < df['Low']) |  # High should never be less than Low\n",
        "            (df['High'] < df['Open']) |  # High should never be less than Open\n",
        "            (df['High'] < df['Close']) |  # High should never be less than Close\n",
        "            (df['Low'] > df['Open']) |   # Low should never be greater than Open\n",
        "            (df['Low'] > df['Close'])     # Low should never be greater than Close\n",
        "        )\n",
        "        \n",
        "        # Remove cases where values are equal (within floating point precision)\n",
        "        equal_cases = (\n",
        "            (abs(df['High'] - df['Low']) < 1e-10) |\n",
        "            (abs(df['High'] - df['Open']) < 1e-10) |\n",
        "            (abs(df['High'] - df['Close']) < 1e-10) |\n",
        "            (abs(df['Low'] - df['Open']) < 1e-10) |\n",
        "            (abs(df['Low'] - df['Close']) < 1e-10)\n",
        "        )\n",
        "        invalid_price = invalid_price & ~equal_cases\n",
        "        \n",
        "        if invalid_price.any():\n",
        "            invalid_rows = df[invalid_price]\n",
        "            print(f\"\\nInvalid price relationships in {name}:\")\n",
        "            print(invalid_rows[['Date', 'Open', 'High', 'Low', 'Close']].head())\n",
        "            raise ValueError(f\"Invalid price relationships ({len(invalid_rows)} rows)\")\n",
        "        \n",
        "        # Check for outliers\n",
        "        for col in ['Open', 'High', 'Low', 'Close']:\n",
        "            mean = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            threshold = VALIDATION_CONFIG['outlier_threshold'] * std\n",
        "            outliers = df[abs(df[col] - mean) > threshold]\n",
        "            if len(outliers) > len(df) * 0.05:  # More than 5% outliers\n",
        "                raise ValueError(f\"High number of outliers in {col}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Data quality validation failed: {str(e)}\")\n",
        "\n",
        "def process_sector_data(df, sector_name, etf):\n",
        "    \"\"\"Process sector ETF data.\"\"\"\n",
        "    try:\n",
        "        # Convert date to datetime with UTC\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        \n",
        "        # Create a new DataFrame with only required columns to avoid SettingWithCopyWarning\n",
        "        processed_df = pd.DataFrame()\n",
        "        for col in REQUIRED_COLUMNS:\n",
        "            processed_df[col] = df[col]\n",
        "        \n",
        "        # Add sector information\n",
        "        processed_df['Sector'] = sector_name\n",
        "        processed_df['Ticker'] = etf\n",
        "        \n",
        "        # Sort by date\n",
        "        processed_df = processed_df.sort_values('Date')\n",
        "        \n",
        "        # Validate data quality\n",
        "        validate_data_quality(processed_df, f\"{sector_name} ({etf})\")\n",
        "        \n",
        "        return processed_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error processing {sector_name} ({etf}): {str(e)}\")\n",
        "\n",
        "def validate_columns(df, required_cols, name):\n",
        "    \"\"\"Validate that all required columns are present.\"\"\"\n",
        "    missing_cols = set(required_cols) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"{name}: Missing columns: {missing_cols}\")\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting sector data processing...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:   0%|          | 0/11 [00:00<?, ?it/s]2025-07-26 07:14:55,711 - INFO - Processing XLK (Information Technology)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "XLK (Information Technology) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:55,773 - INFO - Processing XLF (Financials)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLK.csv\n",
            "\n",
            "XLF (Financials) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:  18%|█▊        | 2/11 [00:00<00:00, 14.96it/s]2025-07-26 07:14:55,847 - INFO - Processing XLV (Healthcare)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLF.csv\n",
            "\n",
            "XLV (Healthcare) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:55,913 - INFO - Processing XLE (Energy)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLV.csv\n",
            "\n",
            "XLE (Energy) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:  36%|███▋      | 4/11 [00:00<00:00, 13.74it/s]2025-07-26 07:14:56,000 - INFO - Processing XLY (Consumer Discretionary)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLE.csv\n",
            "\n",
            "XLY (Consumer Discretionary) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:56,040 - INFO - Processing XLP (Consumer Staples)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLY.csv\n",
            "\n",
            "XLP (Consumer Staples) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:56,069 - INFO - Processing XLI (Industrials)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLP.csv\n",
            "\n",
            "XLI (Industrials) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:56,098 - INFO - Processing XLB (Materials)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLI.csv\n",
            "\n",
            "XLB (Materials) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:  73%|███████▎  | 8/11 [00:00<00:00, 20.56it/s]2025-07-26 07:14:56,138 - INFO - Processing XLU (Utilities)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLB.csv\n",
            "\n",
            "XLU (Utilities) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:56,170 - INFO - Processing XLRE (Real Estate)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-01-02 to 2025-07-25\n",
            "- Trading Days: 2656\n",
            "- File: ..\\data\\sectors\\XLU.csv\n",
            "\n",
            "XLRE (Real Estate) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:14:56,265 - INFO - Processing XLC (Communication Services)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2015-10-08 to 2025-07-25\n",
            "- Trading Days: 2463\n",
            "- File: ..\\data\\sectors\\XLRE.csv\n",
            "\n",
            "XLC (Communication Services) Summary:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors: 100%|██████████| 11/11 [00:00<00:00, 16.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Date Range: 2018-06-19 to 2025-07-25\n",
            "- Trading Days: 1785\n",
            "- File: ..\\data\\sectors\\XLC.csv\n",
            "\n",
            "Sector Coverage:\n",
            "- Total Sectors: 11\n",
            "- Loaded Sectors: 11\n",
            "\n",
            "Creating merged dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2025-07-26 07:14:56,651 - INFO - Saved merged sector data to ..\\data\\processed\\sector_indices_merged.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sector Data Summary:\n",
            "- Total sectors: 11\n",
            "- Date range: 2015-01-02 to 2025-07-25\n",
            "- Trading days: 2656\n",
            "\n",
            "Rows per sector:\n",
            "- Communication Services: 1,785 rows\n",
            "- Consumer Discretionary: 2,656 rows\n",
            "- Consumer Staples: 2,656 rows\n",
            "- Energy: 2,656 rows\n",
            "- Financials: 2,656 rows\n",
            "- Healthcare: 2,656 rows\n",
            "- Industrials: 2,656 rows\n",
            "- Information Technology: 2,656 rows\n",
            "- Materials: 2,656 rows\n",
            "- Real Estate: 2,463 rows\n",
            "- Utilities: 2,656 rows\n",
            "\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "print(\"Starting sector data processing...\\n\")\n",
        "\n",
        "# Dictionary to store processed data\n",
        "sector_data = {}\n",
        "validation_results = {'passed': [], 'failed': []}\n",
        "\n",
        "# Process each sector ETF\n",
        "for etf, sector in tqdm(SECTOR_ETF_MAP.items(), desc=\"Processing sectors\"):\n",
        "    try:\n",
        "        logging.info(f\"Processing {etf} ({sector})\")\n",
        "        \n",
        "        # Load data\n",
        "        file_path = Path(f'../data/sectors/{etf}.csv')\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"No data file found for ETF {etf}\")\n",
        "        \n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Validate columns\n",
        "        validate_columns(df, REQUIRED_COLUMNS, f\"{sector} ({etf})\")\n",
        "        \n",
        "        # Process data\n",
        "        df = process_sector_data(df, sector, etf)\n",
        "        \n",
        "        # Store processed data\n",
        "        sector_data[sector] = df\n",
        "        validation_results['passed'].append(sector)\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"\\n{etf} ({sector}) Summary:\", flush=True)\n",
        "        print(f\"- Date Range: {df['Date'].min():%Y-%m-%d} to {df['Date'].max():%Y-%m-%d}\")\n",
        "        print(f\"- Trading Days: {len(df)}\")\n",
        "        print(f\"- File: {file_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {etf} ({sector}): {str(e)}\")\n",
        "        validation_results['failed'].append(sector)\n",
        "        continue\n",
        "\n",
        "# Print sector coverage\n",
        "print(\"\\nSector Coverage:\", flush=True)\n",
        "print(f\"- Total Sectors: {len(SECTOR_ETF_MAP)}\")\n",
        "print(f\"- Loaded Sectors: {len(validation_results['passed'])}\")\n",
        "if validation_results['failed']:\n",
        "    print(f\"- Failed Sectors: {validation_results['failed']}\")\n",
        "\n",
        "# Create merged dataset if we have data\n",
        "if sector_data:\n",
        "    try:\n",
        "        print(\"\\nCreating merged dataset...\")\n",
        "        merged_data = []\n",
        "        \n",
        "        for sector, df in sector_data.items():\n",
        "            merged_data.append(df)\n",
        "        \n",
        "        merged_df = pd.concat(merged_data, ignore_index=True)\n",
        "        merged_df = merged_df.sort_values(['Date', 'Sector'])\n",
        "        \n",
        "        # Save merged data\n",
        "        output_path = Path('../data/processed/sector_indices_merged.csv')\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        merged_df.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Saved merged sector data to {output_path}\")\n",
        "        \n",
        "        # Print summary\n",
        "        print(\"\\nSector Data Summary:\")\n",
        "        print(f\"- Total sectors: {len(sector_data)}\")\n",
        "        if len(merged_df) > 0:\n",
        "            print(f\"- Date range: {merged_df['Date'].min():%Y-%m-%d} to {merged_df['Date'].max():%Y-%m-%d}\")\n",
        "            print(f\"- Trading days: {len(merged_df['Date'].unique())}\")\n",
        "            print(\"\\nRows per sector:\")\n",
        "            sector_counts = merged_df.groupby('Sector').size()\n",
        "            for sector, count in sector_counts.items():\n",
        "                print(f\"- {sector}: {count:,} rows\")\n",
        "        else:\n",
        "            print(\"No data to summarize\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating merged dataset: {str(e)}\")\n",
        "\n",
        "print(\"\\nProcessing complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
