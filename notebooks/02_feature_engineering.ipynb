{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NASDAQ-100 Technical Indicator Feature Engineering\n",
        "\n",
        "This notebook calculates technical analysis indicators for all NASDAQ-100 stocks and their corresponding sector indices.\n",
        "\n",
        "## Process:\n",
        "1. Load validated NASDAQ-100 data with sector mappings\n",
        "2. Calculate stock-level technical indicators:\n",
        "   - RSI (14-day): Momentum indicator\n",
        "   - EMA (10 and 50-day): Trend following\n",
        "   - MACD (12, 26, 9): Trend and momentum\n",
        "   - OBV (On-Balance Volume): Volume trend\n",
        "   - Bollinger Bands (20-day, 2 std): Volatility\n",
        "   - ADX (14-day): Trend strength\n",
        "   - CCI (20-day): Mean reversion\n",
        "\n",
        "3. Calculate sector-level indicators:\n",
        "   - Same indicators using sector ETF data\n",
        "   - Relative strength vs sector\n",
        "   - Sector momentum indicators\n",
        "   - Cross-sector correlations\n",
        "\n",
        "4. Feature Quality Checks:\n",
        "   - Indicator value distributions\n",
        "   - Missing data analysis\n",
        "   - Correlation analysis\n",
        "   - Sector-wise feature importance\n",
        "\n",
        "5. Output:\n",
        "   - Enriched stock data: `/data/enriched/stocks/{TICKER}_features.csv`\n",
        "   - Enriched sector data: `/data/enriched/sectors/{SECTOR}_features.csv`\n",
        "   - Combined dataset: `/data/enriched/nasdaq_features.csv`\n",
        "   - Feature quality report: `/reports/feature_quality_{timestamp}.log`\n",
        "   - Feature visualizations: `/reports/figures/features/`\n",
        "\n",
        "Note: This notebook uses the validated data from the previous data validation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define expected sectors\n",
        "EXPECTED_SECTORS = {\n",
        "    'Information Technology',\n",
        "    'Financials',\n",
        "    'Healthcare',\n",
        "    'Energy',\n",
        "    'Consumer Discretionary',\n",
        "    'Consumer Staples',\n",
        "    'Industrials',\n",
        "    'Materials',\n",
        "    'Utilities',\n",
        "    'Real Estate',\n",
        "    'Communication Services'\n",
        "}\n",
        "\n",
        "# Load validated data\n",
        "def load_validated_data():\n",
        "    \"\"\"Load validated data from either processed or enriched directory.\"\"\"\n",
        "    try:\n",
        "        # Try processed directory first\n",
        "        processed_path = Path('../data/processed/merged_data.csv')\n",
        "        if processed_path.exists():\n",
        "            df = pd.read_csv(processed_path)\n",
        "            logging.info(f\"Loaded data from {processed_path}\")\n",
        "        else:\n",
        "            # Try enriched directory as fallback\n",
        "            enriched_path = Path('../data/enriched/nasdaq_validated.csv')\n",
        "            if enriched_path.exists():\n",
        "                df = pd.read_csv(enriched_path)\n",
        "                logging.info(f\"Loaded data from {enriched_path}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"No validated data found. Please run data validation notebook first.\")\n",
        "        \n",
        "        # Convert date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        \n",
        "        # Basic data validation\n",
        "        required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Sector', 'Ticker']\n",
        "        missing_cols = set(required_cols) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "            \n",
        "        # Check sector coverage\n",
        "        found_sectors = set(df['Sector'].unique())\n",
        "        missing_sectors = EXPECTED_SECTORS - found_sectors\n",
        "        if missing_sectors:\n",
        "            logging.warning(f\"Missing expected sectors: {missing_sectors}\")\n",
        "            \n",
        "        logging.info(f\"Found {len(found_sectors)} sectors: {sorted(found_sectors)}\")\n",
        "        \n",
        "        # Print sector distribution\n",
        "        for sector in sorted(found_sectors):\n",
        "            tickers = sorted(df[df['Sector'] == sector]['Ticker'].unique())\n",
        "            logging.info(f\"{sector}: {len(tickers)} stocks - {tickers}\")\n",
        "        \n",
        "        logging.info(f\"Successfully loaded data: {len(df)} rows, {df['Ticker'].nunique()} stocks\")\n",
        "        logging.info(f\"Date range: {df['Date'].min():%Y-%m-%d} to {df['Date'].max():%Y-%m-%d}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data: {str(e)}\")\n",
        "        raise Exception(\"Cannot proceed without validated data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Technical indicator calculation functions\n",
        "def calculate_moving_averages(data, periods):\n",
        "    \"\"\"Calculate simple and exponential moving averages.\"\"\"\n",
        "    for period in periods:\n",
        "        data[f'MA_{period}'] = data['Close'].rolling(window=period).mean()\n",
        "        data[f'EMA_{period}'] = data['Close'].ewm(span=period, adjust=False).mean()\n",
        "    return data\n",
        "\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"Calculate Relative Strength Index.\"\"\"\n",
        "    delta = data['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    data['RSI'] = 100 - (100 / (1 + rs))\n",
        "    return data\n",
        "\n",
        "def calculate_bollinger_bands(data, period=20):\n",
        "    \"\"\"Calculate Bollinger Bands.\"\"\"\n",
        "    bb_ma = data['Close'].rolling(window=period).mean()\n",
        "    bb_std = data['Close'].rolling(window=period).std()\n",
        "    data['BB_Upper'] = bb_ma + (bb_std * 2)\n",
        "    data['BB_Lower'] = bb_ma - (bb_std * 2)\n",
        "    data['BB_Width'] = (data['BB_Upper'] - data['BB_Lower']) / bb_ma\n",
        "    return data\n",
        "\n",
        "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
        "    \"\"\"Calculate MACD (Moving Average Convergence Divergence).\"\"\"\n",
        "    exp1 = data['Close'].ewm(span=fast, adjust=False).mean()\n",
        "    exp2 = data['Close'].ewm(span=slow, adjust=False).mean()\n",
        "    data['MACD'] = exp1 - exp2\n",
        "    data['MACD_Signal'] = data['MACD'].ewm(span=signal, adjust=False).mean()\n",
        "    data['MACD_Hist'] = data['MACD'] - data['MACD_Signal']\n",
        "    return data\n",
        "\n",
        "def calculate_atr(data, period=14):\n",
        "    \"\"\"Calculate Average True Range.\"\"\"\n",
        "    high_low = data['High'] - data['Low']\n",
        "    high_close = abs(data['High'] - data['Close'].shift())\n",
        "    low_close = abs(data['Low'] - data['Close'].shift())\n",
        "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "    true_range = ranges.max(axis=1)\n",
        "    data['ATR'] = true_range.rolling(window=period).mean()\n",
        "    return data\n",
        "\n",
        "def calculate_volume_indicators(data, period=20):\n",
        "    \"\"\"Calculate volume-based indicators.\"\"\"\n",
        "    # On-Balance Volume\n",
        "    data['OBV'] = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()\n",
        "    \n",
        "    # Volume Moving Average and Ratio\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=period).mean()\n",
        "    data['Volume_Ratio'] = data['Volume'] / data['Volume_MA']\n",
        "    return data\n",
        "\n",
        "def calculate_momentum_indicators(data, period=14):\n",
        "    \"\"\"Calculate momentum indicators.\"\"\"\n",
        "    # Rate of Change\n",
        "    data['ROC'] = data['Close'].pct_change(periods=period) * 100\n",
        "    \n",
        "    # Stochastic Oscillator\n",
        "    low_min = data['Low'].rolling(window=period).min()\n",
        "    high_max = data['High'].rolling(window=period).max()\n",
        "    data['%K'] = ((data['Close'] - low_min) / (high_max - low_min)) * 100\n",
        "    data['%D'] = data['%K'].rolling(window=3).mean()\n",
        "    return data\n",
        "\n",
        "def calculate_price_indicators(data):\n",
        "    \"\"\"Calculate price-based indicators.\"\"\"\n",
        "    data['Daily_Return'] = data['Close'].pct_change()\n",
        "    data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Return'].rolling(window=20).std() * np.sqrt(252)\n",
        "    return data\n",
        "\n",
        "def calculate_sector_indicators(data):\n",
        "    \"\"\"Calculate sector-relative indicators.\"\"\"\n",
        "    if 'Sector_Close' in data.columns:\n",
        "        data['Sector_Return'] = data['Sector_Close'].pct_change()\n",
        "        data['Relative_Strength'] = data['Close'] / data['Sector_Close']\n",
        "        data['RS_MA'] = data['Relative_Strength'].rolling(window=20).mean()\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main feature engineering functions\n",
        "def calculate_all_features(df, config):\n",
        "    \"\"\"Calculate all technical indicators for a given stock/sector.\"\"\"\n",
        "    try:\n",
        "        # Make a copy to avoid modifying original data\n",
        "        data = df.copy()\n",
        "        \n",
        "        # Sort by date to ensure correct calculations\n",
        "        data = data.sort_values('Date')\n",
        "        \n",
        "        # Calculate all indicators\n",
        "        data = calculate_moving_averages(data, config['ma_periods'])\n",
        "        data = calculate_rsi(data, config['rsi_period'])\n",
        "        data = calculate_bollinger_bands(data, config['bb_period'])\n",
        "        data = calculate_macd(data, \n",
        "                            fast=config['macd_params']['fast'],\n",
        "                            slow=config['macd_params']['slow'],\n",
        "                            signal=config['macd_params']['signal'])\n",
        "        data = calculate_atr(data, config['atr_period'])\n",
        "        data = calculate_volume_indicators(data, config['obv_period'])\n",
        "        data = calculate_momentum_indicators(data, config['roc_period'])\n",
        "        data = calculate_price_indicators(data)\n",
        "        data = calculate_sector_indicators(data)\n",
        "        \n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating features: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def process_stock_data(df, ticker, config):\n",
        "    \"\"\"Process data for a single stock.\"\"\"\n",
        "    try:\n",
        "        # Get stock data\n",
        "        stock_df = df[df['Ticker'] == ticker].copy()\n",
        "        if len(stock_df) == 0:\n",
        "            raise ValueError(f\"No data found for ticker {ticker}\")\n",
        "        \n",
        "        # Calculate features\n",
        "        stock_df = calculate_all_features(stock_df, config)\n",
        "        \n",
        "        # Calculate feature statistics\n",
        "        feature_stats = {\n",
        "            'ticker': ticker,\n",
        "            'sector': stock_df['Sector'].iloc[0],\n",
        "            'start_date': stock_df['Date'].min(),\n",
        "            'end_date': stock_df['Date'].max(),\n",
        "            'trading_days': len(stock_df),\n",
        "            'avg_volume': stock_df['Volume'].mean(),\n",
        "            'avg_volatility': stock_df['Volatility'].mean(),\n",
        "            'sharpe_ratio': stock_df['Daily_Return'].mean() / stock_df['Daily_Return'].std() * np.sqrt(252),\n",
        "            'correlation_with_sector': stock_df['Daily_Return'].corr(stock_df['Sector_Return'])\n",
        "        }\n",
        "        \n",
        "        return stock_df, feature_stats\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:35:57,089 - INFO - Loaded data from ..\\data\\processed\\merged_data.csv\n",
            "2025-07-26 07:35:57,205 - INFO - Found 11 sectors: ['Communication Services', 'Consumer Discretionary', 'Consumer Staples', 'Energy', 'Financials', 'Healthcare', 'Industrials', 'Information Technology', 'Materials', 'Real Estate', 'Utilities']\n",
            "2025-07-26 07:35:57,249 - INFO - Communication Services: 6 stocks - ['CHTR', 'CMCSA', 'NFLX', 'SIRI', 'TMUS', 'WBD']\n",
            "2025-07-26 07:35:57,295 - INFO - Consumer Discretionary: 14 stocks - ['ABNB', 'AMZN', 'BKNG', 'CVNA', 'DLTR', 'EBAY', 'LCID', 'MAR', 'MELI', 'ORLY', 'RIVN', 'ROST', 'SBUX', 'TSLA']\n",
            "2025-07-26 07:35:57,331 - INFO - Consumer Staples: 6 stocks - ['COST', 'KDP', 'KHC', 'MDLZ', 'MNST', 'PEP']\n",
            "2025-07-26 07:35:57,364 - INFO - Energy: 2 stocks - ['BKR', 'FANG']\n",
            "2025-07-26 07:35:57,399 - INFO - Financials: 2 stocks - ['COIN', 'PYPL']\n",
            "2025-07-26 07:35:57,449 - INFO - Healthcare: 11 stocks - ['ALGN', 'AMGN', 'BIIB', 'DXCM', 'GILD', 'IDXX', 'ILMN', 'ISRG', 'MRNA', 'REGN', 'VRTX']\n",
            "2025-07-26 07:35:57,491 - INFO - Industrials: 7 stocks - ['CPRT', 'CSX', 'CTAS', 'FAST', 'HON', 'ODFL', 'PCAR']\n",
            "2025-07-26 07:35:57,561 - INFO - Information Technology: 51 stocks - ['AAPL', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AMAT', 'AMD', 'ANSS', 'ASML', 'AVGO', 'CDNS', 'CRWD', 'CSCO', 'CTSH', 'DASH', 'DDOG', 'DOCU', 'EA', 'ENPH', 'FTNT', 'GFS', 'GOOG', 'GOOGL', 'INTC', 'INTU', 'KLAC', 'LRCX', 'MCHP', 'META', 'MRVL', 'MSFT', 'MTCH', 'MU', 'NVDA', 'NXPI', 'OKTA', 'ON', 'PANW', 'PAYX', 'QCOM', 'RBLX', 'ROKU', 'SNAP', 'SNPS', 'TEAM', 'TTD', 'TXN', 'VRSK', 'WDAY', 'ZM', 'ZS']\n",
            "2025-07-26 07:35:57,596 - INFO - Materials: 3 stocks - ['APD', 'FCX', 'LIN']\n",
            "2025-07-26 07:35:57,634 - INFO - Real Estate: 3 stocks - ['CSGP', 'EQIX', 'PLD']\n",
            "2025-07-26 07:35:57,670 - INFO - Utilities: 3 stocks - ['AEP', 'EXC', 'XEL']\n",
            "2025-07-26 07:35:57,693 - INFO - Successfully loaded data: 266029 rows, 108 stocks\n",
            "2025-07-26 07:35:57,699 - INFO - Date range: 2015-01-02 to 2025-07-25\n",
            "2025-07-26 07:35:57,853 - INFO - Found 11 sectors: ['Communication Services', 'Consumer Discretionary', 'Consumer Staples', 'Energy', 'Financials', 'Healthcare', 'Industrials', 'Information Technology', 'Materials', 'Real Estate', 'Utilities']\n",
            "2025-07-26 07:35:57,882 - INFO - Communication Services: 6 stocks - ['CHTR', 'CMCSA', 'NFLX', 'SIRI', 'TMUS', 'WBD']\n",
            "2025-07-26 07:35:57,906 - INFO - Consumer Discretionary: 14 stocks - ['ABNB', 'AMZN', 'BKNG', 'CVNA', 'DLTR', 'EBAY', 'LCID', 'MAR', 'MELI', 'ORLY', 'RIVN', 'ROST', 'SBUX', 'TSLA']\n",
            "2025-07-26 07:35:57,931 - INFO - Consumer Staples: 6 stocks - ['COST', 'KDP', 'KHC', 'MDLZ', 'MNST', 'PEP']\n",
            "2025-07-26 07:35:57,971 - INFO - Energy: 2 stocks - ['BKR', 'FANG']\n",
            "2025-07-26 07:35:58,021 - INFO - Financials: 2 stocks - ['COIN', 'PYPL']\n",
            "2025-07-26 07:35:58,071 - INFO - Healthcare: 11 stocks - ['ALGN', 'AMGN', 'BIIB', 'DXCM', 'GILD', 'IDXX', 'ILMN', 'ISRG', 'MRNA', 'REGN', 'VRTX']\n",
            "2025-07-26 07:35:58,129 - INFO - Industrials: 7 stocks - ['CPRT', 'CSX', 'CTAS', 'FAST', 'HON', 'ODFL', 'PCAR']\n",
            "2025-07-26 07:35:58,245 - INFO - Information Technology: 51 stocks - ['AAPL', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AMAT', 'AMD', 'ANSS', 'ASML', 'AVGO', 'CDNS', 'CRWD', 'CSCO', 'CTSH', 'DASH', 'DDOG', 'DOCU', 'EA', 'ENPH', 'FTNT', 'GFS', 'GOOG', 'GOOGL', 'INTC', 'INTU', 'KLAC', 'LRCX', 'MCHP', 'META', 'MRVL', 'MSFT', 'MTCH', 'MU', 'NVDA', 'NXPI', 'OKTA', 'ON', 'PANW', 'PAYX', 'QCOM', 'RBLX', 'ROKU', 'SNAP', 'SNPS', 'TEAM', 'TTD', 'TXN', 'VRSK', 'WDAY', 'ZM', 'ZS']\n",
            "2025-07-26 07:35:58,281 - INFO - Materials: 3 stocks - ['APD', 'FCX', 'LIN']\n",
            "2025-07-26 07:35:58,316 - INFO - Real Estate: 3 stocks - ['CSGP', 'EQIX', 'PLD']\n",
            "2025-07-26 07:35:58,352 - INFO - Utilities: 3 stocks - ['AEP', 'EXC', 'XEL']\n"
          ]
        }
      ],
      "source": [
        "# Load data and prepare for processing\n",
        "try:\n",
        "    # Load the validated data\n",
        "    df = load_validated_data()\n",
        "    \n",
        "    # Configuration for feature engineering\n",
        "    FEATURE_CONFIG = {\n",
        "        'price_features': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "        'ma_periods': [5, 10, 20, 50, 100, 200],  # Moving average periods\n",
        "        'rsi_period': 14,  # RSI calculation period\n",
        "        'bb_period': 20,   # Bollinger Bands period\n",
        "        'macd_params': {\n",
        "            'fast': 12,    # Fast EMA period\n",
        "            'slow': 26,    # Slow EMA period\n",
        "            'signal': 9    # Signal line period\n",
        "        },\n",
        "        'atr_period': 14,  # Average True Range period\n",
        "        'obv_period': 20,  # On-Balance Volume period\n",
        "        'roc_period': 12,  # Rate of Change period\n",
        "        'stoch_params': {\n",
        "            'k_period': 14,  # %K period\n",
        "            'k_smooth': 3,   # %K smoothing\n",
        "            'd_period': 3    # %D period\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Define sector ETF mapping\n",
        "    SECTOR_ETF_MAP = {\n",
        "        'XLK': 'Information Technology',\n",
        "        'XLF': 'Financials',\n",
        "        'XLV': 'Healthcare',\n",
        "        'XLE': 'Energy',\n",
        "        'XLY': 'Consumer Discretionary',\n",
        "        'XLP': 'Consumer Staples',\n",
        "        'XLI': 'Industrials',\n",
        "        'XLB': 'Materials',\n",
        "        'XLU': 'Utilities',\n",
        "        'XLRE': 'Real Estate',\n",
        "        'XLC': 'Communication Services'\n",
        "    }\n",
        "    \n",
        "    # Clean and standardize sector names in the dataframe\n",
        "    df['Sector'] = df['Sector'].str.strip()  # Remove trailing/leading spaces\n",
        "    \n",
        "    # Map old sector names to standardized names\n",
        "    SECTOR_NAME_MAP = {\n",
        "        'Technology': 'Information Technology',\n",
        "        'Technology ': 'Information Technology',  # Handle the case with extra space\n",
        "        'Consumer_Discretionary': 'Consumer Discretionary',\n",
        "        'Consumer_Staples': 'Consumer Staples',\n",
        "        'Communication_Services': 'Communication Services'\n",
        "    }\n",
        "    \n",
        "    # Apply sector name standardization\n",
        "    df['Sector'] = df['Sector'].replace(SECTOR_NAME_MAP)\n",
        "    \n",
        "    # Get unique sectors and validate\n",
        "    sectors = sorted(df['Sector'].unique())\n",
        "    expected_sectors = set(SECTOR_ETF_MAP.values())\n",
        "    missing_sectors = expected_sectors - set(sectors)\n",
        "    extra_sectors = set(sectors) - expected_sectors\n",
        "    \n",
        "    if missing_sectors:\n",
        "        logging.warning(f\"Missing expected sectors: {missing_sectors}\")\n",
        "    if extra_sectors:\n",
        "        logging.warning(f\"Found unexpected sectors: {extra_sectors}\")\n",
        "    \n",
        "    logging.info(f\"Found {len(sectors)} sectors: {sectors}\")\n",
        "    \n",
        "    # Get tickers by sector\n",
        "    sector_tickers = {}\n",
        "    for sector in sectors:\n",
        "        tickers = sorted(df[df['Sector'] == sector]['Ticker'].unique())\n",
        "        sector_tickers[sector] = tickers\n",
        "        logging.info(f\"{sector}: {len(tickers)} stocks - {tickers}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logging.error(f\"Error in data preparation: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sectors:   0%|          | 0/11 [00:00<?, ?it/s]2025-07-26 07:35:58,419 - INFO - \n",
            "Processing sector: Communication Services\n",
            "2025-07-26 07:36:00,609 - INFO - Saved Communication Services features to ..\\data\\enriched\\sectors\\Communication_Services_features.csv\n",
            "Processing sectors:   9%|▉         | 1/11 [00:02<00:21,  2.19s/it]2025-07-26 07:36:00,611 - INFO - \n",
            "Processing sector: Consumer Discretionary\n",
            "2025-07-26 07:36:04,556 - INFO - Saved Consumer Discretionary features to ..\\data\\enriched\\sectors\\Consumer_Discretionary_features.csv\n",
            "Processing sectors:  18%|█▊        | 2/11 [00:06<00:29,  3.22s/it]2025-07-26 07:36:04,559 - INFO - \n",
            "Processing sector: Consumer Staples\n",
            "2025-07-26 07:36:06,447 - INFO - Saved Consumer Staples features to ..\\data\\enriched\\sectors\\Consumer_Staples_features.csv\n",
            "Processing sectors:  27%|██▋       | 3/11 [00:08<00:20,  2.62s/it]2025-07-26 07:36:06,449 - INFO - \n",
            "Processing sector: Energy\n",
            "2025-07-26 07:36:07,091 - INFO - Saved Energy features to ..\\data\\enriched\\sectors\\Energy_features.csv\n",
            "Processing sectors:  36%|███▋      | 4/11 [00:08<00:12,  1.84s/it]2025-07-26 07:36:07,094 - INFO - \n",
            "Processing sector: Financials\n",
            "2025-07-26 07:36:07,500 - INFO - Saved Financials features to ..\\data\\enriched\\sectors\\Financials_features.csv\n",
            "Processing sectors:  45%|████▌     | 5/11 [00:09<00:07,  1.32s/it]2025-07-26 07:36:07,502 - INFO - \n",
            "Processing sector: Healthcare\n",
            "2025-07-26 07:36:10,368 - INFO - Saved Healthcare features to ..\\data\\enriched\\sectors\\Healthcare_features.csv\n",
            "Processing sectors:  55%|█████▍    | 6/11 [00:11<00:09,  1.85s/it]2025-07-26 07:36:10,370 - INFO - \n",
            "Processing sector: Industrials\n",
            "2025-07-26 07:36:12,510 - INFO - Saved Industrials features to ..\\data\\enriched\\sectors\\Industrials_features.csv\n",
            "Processing sectors:  64%|██████▎   | 7/11 [00:14<00:07,  1.94s/it]2025-07-26 07:36:12,512 - INFO - \n",
            "Processing sector: Information Technology\n",
            "2025-07-26 07:36:26,985 - INFO - Saved Information Technology features to ..\\data\\enriched\\sectors\\Information_Technology_features.csv\n",
            "Processing sectors:  73%|███████▎  | 8/11 [00:28<00:17,  5.93s/it]2025-07-26 07:36:26,987 - INFO - \n",
            "Processing sector: Materials\n",
            "2025-07-26 07:36:27,892 - INFO - Saved Materials features to ..\\data\\enriched\\sectors\\Materials_features.csv\n",
            "Processing sectors:  82%|████████▏ | 9/11 [00:29<00:08,  4.36s/it]2025-07-26 07:36:27,893 - INFO - \n",
            "Processing sector: Real Estate\n",
            "2025-07-26 07:36:28,739 - INFO - Saved Real Estate features to ..\\data\\enriched\\sectors\\Real_Estate_features.csv\n",
            "Processing sectors:  91%|█████████ | 10/11 [00:30<00:03,  3.28s/it]2025-07-26 07:36:28,740 - INFO - \n",
            "Processing sector: Utilities\n",
            "2025-07-26 07:36:29,657 - INFO - Saved Utilities features to ..\\data\\enriched\\sectors\\Utilities_features.csv\n",
            "Processing sectors: 100%|██████████| 11/11 [00:31<00:00,  2.84s/it]\n",
            "2025-07-26 07:36:29,665 - INFO - \n",
            "Feature engineering complete!\n",
            "2025-07-26 07:36:29,666 - INFO - Processed 108 stocks across 11 sectors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Stock distribution by sector:\n",
            "- Communication Services: 6 stocks\n",
            "- Consumer Discretionary: 14 stocks\n",
            "- Consumer Staples: 6 stocks\n",
            "- Energy: 2 stocks\n",
            "- Financials: 2 stocks\n",
            "- Healthcare: 11 stocks\n",
            "- Industrials: 7 stocks\n",
            "- Information Technology: 51 stocks\n",
            "- Materials: 3 stocks\n",
            "- Real Estate: 3 stocks\n",
            "- Utilities: 3 stocks\n"
          ]
        }
      ],
      "source": [
        "# Process data and save results\n",
        "try:\n",
        "    # Create output directories if they don't exist\n",
        "    enriched_dir = Path('../data/enriched')\n",
        "    stocks_dir = enriched_dir / 'stocks'\n",
        "    sectors_dir = enriched_dir / 'sectors'\n",
        "    for dir_path in [enriched_dir, stocks_dir, sectors_dir]:\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Process stocks by sector\n",
        "    feature_stats = {'stocks': [], 'sectors': []}\n",
        "    \n",
        "    for sector in tqdm(sectors, desc=\"Processing sectors\"):\n",
        "        try:\n",
        "            logging.info(f\"\\nProcessing sector: {sector}\")\n",
        "            sector_ticks = sector_tickers[sector]\n",
        "            \n",
        "            # Skip if no tickers in sector\n",
        "            if not sector_ticks:\n",
        "                logging.warning(f\"No stocks found for sector {sector}, skipping...\")\n",
        "                continue\n",
        "            \n",
        "            # Process each stock in the sector\n",
        "            sector_data = []\n",
        "            for ticker in tqdm(sector_ticks, desc=f\"Processing {sector} stocks\", leave=False):\n",
        "                stock_df, stats = process_stock_data(df, ticker, FEATURE_CONFIG)\n",
        "                if stock_df is not None:\n",
        "                    sector_data.append(stock_df)\n",
        "                    feature_stats['stocks'].append(stats)\n",
        "            \n",
        "            if sector_data:\n",
        "                # Combine sector data\n",
        "                sector_df = pd.concat(sector_data, ignore_index=True)\n",
        "                \n",
        "                # Save individual stock files\n",
        "                for ticker in sector_ticks:\n",
        "                    stock_data = sector_df[sector_df['Ticker'] == ticker]\n",
        "                    stock_file = stocks_dir / f\"{ticker}_features.csv\"\n",
        "                    stock_data.to_csv(stock_file, index=False)\n",
        "                \n",
        "                # Save sector data\n",
        "                sector_file = sectors_dir / f\"{sector.replace(' ', '_')}_features.csv\"\n",
        "                sector_df.to_csv(sector_file, index=False)\n",
        "                logging.info(f\"Saved {sector} features to {sector_file}\")\n",
        "                \n",
        "                # Calculate sector statistics\n",
        "                sector_stats = {\n",
        "                    'sector': sector,\n",
        "                    'num_stocks': len(sector_ticks),\n",
        "                    'avg_correlation': np.mean([s['correlation_with_sector'] for s in feature_stats['stocks'] if s['sector'] == sector]),\n",
        "                    'avg_volatility': np.mean([s['avg_volatility'] for s in feature_stats['stocks'] if s['sector'] == sector]),\n",
        "                    'avg_sharpe': np.mean([s['sharpe_ratio'] for s in feature_stats['stocks'] if s['sector'] == sector])\n",
        "                }\n",
        "                feature_stats['sectors'].append(sector_stats)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing sector {sector}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    # Save feature statistics\n",
        "    stats_df = pd.DataFrame(feature_stats['stocks'])\n",
        "    sector_stats_df = pd.DataFrame(feature_stats['sectors'])\n",
        "    \n",
        "    stats_df.to_csv(stocks_dir / 'stock_statistics.csv', index=False)\n",
        "    sector_stats_df.to_csv(sectors_dir / 'sector_statistics.csv', index=False)\n",
        "    \n",
        "    logging.info(\"\\nFeature engineering complete!\")\n",
        "    logging.info(f\"Processed {len(feature_stats['stocks'])} stocks across {len(feature_stats['sectors'])} sectors\")\n",
        "    \n",
        "    # Print sector coverage summary\n",
        "    processed_sectors = set(s['sector'] for s in feature_stats['sectors'])\n",
        "    missing_sectors = expected_sectors - processed_sectors\n",
        "    if missing_sectors:\n",
        "        logging.warning(f\"Sectors with no processed data: {missing_sectors}\")\n",
        "    \n",
        "    # Print stock distribution\n",
        "    print(\"\\nStock distribution by sector:\")\n",
        "    for sector_stats in sorted(feature_stats['sectors'], key=lambda x: x['sector']):\n",
        "        print(f\"- {sector_stats['sector']}: {sector_stats['num_stocks']} stocks\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logging.error(f\"Error in main processing: {str(e)}\")\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
