{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# [DEPRECATED] NASDAQ-100 Stock Data Collection\n",
        "\n",
        "⚠️ **DEPRECATION NOTICE**:\n",
        "This notebook is deprecated and has been replaced by newer, more robust data collection notebooks:\n",
        "\n",
        "1. Use `01b_download_nasdaq_top100.ipynb` for downloading NASDAQ-100 stock data\n",
        "2. Use `01c_download_sector_indices.ipynb` for sector ETF data\n",
        "3. Use `01d_merge_and_validate_data.ipynb` for data validation and merging\n",
        "\n",
        "Please refer to these notebooks for up-to-date data collection processes.\n",
        "\n",
        "## Original Specifications (For Reference Only):\n",
        "- Timeframe: 2015-01-01 to present\n",
        "- Interval: Daily ('1d')\n",
        "- Fields: OHLCV, Dividends, Stock Splits\n",
        "- Universe: NASDAQ-100 stocks with sector classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 04:57:35,306 - INFO - Data collection configured for period: 2015-01-01 to 2025-07-21\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs will be saved to: ../reports/data_collection_20250726_045735.log\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "log_file = f'../reports/data_collection_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create necessary directories\n",
        "Path('../data/stocks').mkdir(parents=True, exist_ok=True)\n",
        "Path('../reports').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define parameters\n",
        "START_DATE = '2015-01-01'\n",
        "END_DATE = '2025-07-21'\n",
        "INTERVAL = '1d'\n",
        "\n",
        "# Required columns for validation\n",
        "REQUIRED_COLUMNS = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
        "\n",
        "logging.info(f\"Data collection configured for period: {START_DATE} to {END_DATE}\")\n",
        "print(f\"Logs will be saved to: {log_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 04:57:35,388 - INFO - Loaded 98 tickers from sector mapping\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sector distribution:\n",
            "- Technology: 50 stocks\n",
            "- Consumer Discretionary: 12 stocks\n",
            "- Healthcare: 11 stocks\n",
            "- Industrials: 7 stocks\n",
            "- Consumer Staples: 6 stocks\n",
            "- Communication Services: 6 stocks\n",
            "- Utilities: 3 stocks\n",
            "- Energy: 2 stocks\n",
            "- Technology : 1 stocks\n"
          ]
        }
      ],
      "source": [
        "# Load sector mapping\n",
        "try:\n",
        "    sector_mapping = pd.read_csv('../data/sector_mapping.csv')\n",
        "    tickers = sector_mapping['Ticker'].unique()\n",
        "    logging.info(f\"Loaded {len(tickers)} tickers from sector mapping\")\n",
        "    \n",
        "    print(\"\\nSector distribution:\")\n",
        "    sector_counts = sector_mapping['Sector'].value_counts()\n",
        "    for sector, count in sector_counts.items():\n",
        "        print(f\"- {sector}: {count} stocks\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading sector mapping: {str(e)}\")\n",
        "    raise Exception(\"Cannot proceed without sector mapping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing stocks:   0%|          | 0/98 [00:00<?, ?it/s]2025-07-26 04:57:35,481 - INFO - Processing AAPL\n",
            "2025-07-26 04:57:35,498 - INFO - Loaded existing data for AAPL\n",
            "2025-07-26 04:57:35,499 - INFO - Processing MSFT\n",
            "2025-07-26 04:57:35,510 - INFO - Loaded existing data for MSFT\n",
            "2025-07-26 04:57:35,511 - INFO - Processing AMZN\n",
            "2025-07-26 04:57:35,522 - INFO - Loaded existing data for AMZN\n",
            "2025-07-26 04:57:35,523 - INFO - Processing NVDA\n",
            "2025-07-26 04:57:35,537 - INFO - Loaded existing data for NVDA\n",
            "2025-07-26 04:57:35,539 - INFO - Processing META\n",
            "2025-07-26 04:57:35,551 - INFO - Loaded existing data for META\n",
            "2025-07-26 04:57:35,553 - INFO - Processing GOOGL\n",
            "2025-07-26 04:57:35,569 - INFO - Loaded existing data for GOOGL\n",
            "2025-07-26 04:57:35,570 - INFO - Processing GOOG\n",
            "2025-07-26 04:57:35,581 - INFO - Loaded existing data for GOOG\n",
            "Processing stocks:   7%|▋         | 7/98 [00:00<00:01, 69.31it/s]2025-07-26 04:57:35,583 - INFO - Processing TSLA\n",
            "2025-07-26 04:57:35,593 - INFO - Loaded existing data for TSLA\n",
            "2025-07-26 04:57:35,594 - INFO - Processing AVGO\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 04:57:35,603 - INFO - Loaded existing data for AVGO\n",
            "2025-07-26 04:57:35,604 - INFO - Processing PEP\n",
            "2025-07-26 04:57:35,613 - INFO - Loaded existing data for PEP\n",
            "2025-07-26 04:57:35,614 - INFO - Processing COST\n",
            "2025-07-26 04:57:35,623 - INFO - Loaded existing data for COST\n",
            "2025-07-26 04:57:35,625 - INFO - Processing CSCO\n",
            "2025-07-26 04:57:35,636 - INFO - Loaded existing data for CSCO\n",
            "2025-07-26 04:57:35,638 - INFO - Processing TMUS\n",
            "2025-07-26 04:57:35,650 - INFO - Loaded existing data for TMUS\n",
            "2025-07-26 04:57:35,651 - INFO - Processing ADBE\n",
            "2025-07-26 04:57:35,661 - INFO - Loaded existing data for ADBE\n",
            "2025-07-26 04:57:35,663 - INFO - Processing NFLX\n",
            "2025-07-26 04:57:35,673 - INFO - Loaded existing data for NFLX\n",
            "2025-07-26 04:57:35,674 - INFO - Processing CMCSA\n",
            "2025-07-26 04:57:35,684 - INFO - Loaded existing data for CMCSA\n",
            "Processing stocks:  16%|█▋        | 16/98 [00:00<00:01, 80.37it/s]2025-07-26 04:57:35,686 - INFO - Processing AMD\n",
            "2025-07-26 04:57:35,698 - INFO - Loaded existing data for AMD\n",
            "2025-07-26 04:57:35,700 - INFO - Processing TXN\n",
            "2025-07-26 04:57:35,711 - INFO - Loaded existing data for TXN\n",
            "2025-07-26 04:57:35,713 - INFO - Processing INTC\n",
            "2025-07-26 04:57:35,726 - INFO - Loaded existing data for INTC\n",
            "2025-07-26 04:57:35,729 - INFO - Processing QCOM\n",
            "2025-07-26 04:57:35,740 - INFO - Loaded existing data for QCOM\n",
            "2025-07-26 04:57:35,742 - INFO - Processing INTU\n",
            "2025-07-26 04:57:35,755 - INFO - Loaded existing data for INTU\n",
            "2025-07-26 04:57:35,757 - INFO - Processing AMGN\n",
            "2025-07-26 04:57:35,769 - INFO - Loaded existing data for AMGN\n",
            "2025-07-26 04:57:35,771 - INFO - Processing HON\n",
            "2025-07-26 04:57:35,780 - INFO - Loaded existing data for HON\n",
            "2025-07-26 04:57:35,781 - INFO - Processing AMAT\n",
            "2025-07-26 04:57:35,792 - INFO - Loaded existing data for AMAT\n",
            "2025-07-26 04:57:35,793 - INFO - Processing ISRG\n",
            "2025-07-26 04:57:35,803 - INFO - Loaded existing data for ISRG\n",
            "Processing stocks:  26%|██▌       | 25/98 [00:00<00:00, 77.98it/s]2025-07-26 04:57:35,805 - INFO - Processing SBUX\n",
            "2025-07-26 04:57:35,814 - INFO - Loaded existing data for SBUX\n",
            "2025-07-26 04:57:35,816 - INFO - Processing ADP\n",
            "2025-07-26 04:57:35,826 - INFO - Loaded existing data for ADP\n",
            "2025-07-26 04:57:35,827 - INFO - Processing MDLZ\n",
            "2025-07-26 04:57:35,837 - INFO - Loaded existing data for MDLZ\n",
            "2025-07-26 04:57:35,838 - INFO - Processing ADI\n",
            "2025-07-26 04:57:35,849 - INFO - Loaded existing data for ADI\n",
            "2025-07-26 04:57:35,850 - INFO - Processing BKNG\n",
            "2025-07-26 04:57:35,861 - INFO - Loaded existing data for BKNG\n",
            "2025-07-26 04:57:35,862 - INFO - Processing GILD\n",
            "2025-07-26 04:57:35,872 - INFO - Loaded existing data for GILD\n",
            "2025-07-26 04:57:35,873 - INFO - Processing REGN\n",
            "2025-07-26 04:57:35,882 - INFO - Loaded existing data for REGN\n",
            "2025-07-26 04:57:35,883 - INFO - Processing VRTX\n",
            "2025-07-26 04:57:35,893 - INFO - Loaded existing data for VRTX\n",
            "2025-07-26 04:57:35,894 - INFO - Processing PANW\n",
            "2025-07-26 04:57:35,903 - INFO - Loaded existing data for PANW\n",
            "Processing stocks:  35%|███▍      | 34/98 [00:00<00:00, 82.22it/s]2025-07-26 04:57:35,905 - INFO - Processing SNPS\n",
            "2025-07-26 04:57:35,915 - INFO - Loaded existing data for SNPS\n",
            "2025-07-26 04:57:35,916 - INFO - Processing CDNS\n",
            "2025-07-26 04:57:35,927 - INFO - Loaded existing data for CDNS\n",
            "2025-07-26 04:57:35,928 - INFO - Processing KLAC\n",
            "2025-07-26 04:57:35,938 - INFO - Loaded existing data for KLAC\n",
            "2025-07-26 04:57:35,940 - INFO - Processing MELI\n",
            "2025-07-26 04:57:35,949 - INFO - Loaded existing data for MELI\n",
            "2025-07-26 04:57:35,950 - INFO - Processing LRCX\n",
            "2025-07-26 04:57:35,963 - INFO - Loaded existing data for LRCX\n",
            "2025-07-26 04:57:35,965 - INFO - Processing MU\n",
            "2025-07-26 04:57:35,977 - INFO - Loaded existing data for MU\n",
            "2025-07-26 04:57:35,979 - INFO - Processing ASML\n",
            "2025-07-26 04:57:35,990 - INFO - Loaded existing data for ASML\n",
            "2025-07-26 04:57:35,991 - INFO - Processing CHTR\n",
            "2025-07-26 04:57:36,003 - INFO - Loaded existing data for CHTR\n",
            "2025-07-26 04:57:36,005 - INFO - Processing MAR\n",
            "2025-07-26 04:57:36,014 - INFO - Loaded existing data for MAR\n",
            "Processing stocks:  44%|████▍     | 43/98 [00:00<00:00, 81.54it/s]2025-07-26 04:57:36,019 - INFO - Processing ORLY\n",
            "2025-07-26 04:57:36,030 - INFO - Loaded existing data for ORLY\n",
            "2025-07-26 04:57:36,030 - INFO - Processing MNST\n",
            "2025-07-26 04:57:36,039 - INFO - Loaded existing data for MNST\n",
            "2025-07-26 04:57:36,041 - INFO - Processing FTNT\n",
            "2025-07-26 04:57:36,049 - INFO - Loaded existing data for FTNT\n",
            "2025-07-26 04:57:36,050 - INFO - Processing CTAS\n",
            "2025-07-26 04:57:36,059 - INFO - Loaded existing data for CTAS\n",
            "2025-07-26 04:57:36,060 - INFO - Processing PAYX\n",
            "2025-07-26 04:57:36,069 - INFO - Loaded existing data for PAYX\n",
            "2025-07-26 04:57:36,070 - INFO - Processing MRVL\n",
            "2025-07-26 04:57:36,079 - INFO - Loaded existing data for MRVL\n",
            "2025-07-26 04:57:36,080 - INFO - Processing ABNB\n",
            "2025-07-26 04:57:36,089 - INFO - Loaded existing data for ABNB\n",
            "2025-07-26 04:57:36,090 - INFO - Processing ADSK\n",
            "2025-07-26 04:57:36,100 - INFO - Loaded existing data for ADSK\n",
            "2025-07-26 04:57:36,102 - INFO - Processing ODFL\n",
            "2025-07-26 04:57:36,110 - INFO - Loaded existing data for ODFL\n",
            "2025-07-26 04:57:36,112 - INFO - Processing IDXX\n",
            "2025-07-26 04:57:36,123 - INFO - Loaded existing data for IDXX\n",
            "Processing stocks:  54%|█████▍    | 53/98 [00:00<00:00, 85.32it/s]2025-07-26 04:57:36,125 - INFO - Processing BIIB\n",
            "2025-07-26 04:57:36,134 - INFO - Loaded existing data for BIIB\n",
            "2025-07-26 04:57:36,136 - INFO - Processing KDP\n",
            "2025-07-26 04:57:36,147 - INFO - Loaded existing data for KDP\n",
            "2025-07-26 04:57:36,148 - INFO - Processing KHC\n",
            "2025-07-26 04:57:36,159 - INFO - Loaded existing data for KHC\n",
            "2025-07-26 04:57:36,161 - INFO - Processing DXCM\n",
            "2025-07-26 04:57:36,173 - INFO - Loaded existing data for DXCM\n",
            "2025-07-26 04:57:36,174 - INFO - Processing NXPI\n",
            "2025-07-26 04:57:36,183 - INFO - Loaded existing data for NXPI\n",
            "2025-07-26 04:57:36,185 - INFO - Processing MCHP\n",
            "2025-07-26 04:57:36,194 - INFO - Loaded existing data for MCHP\n",
            "2025-07-26 04:57:36,196 - INFO - Processing WDAY\n",
            "2025-07-26 04:57:36,208 - INFO - Loaded existing data for WDAY\n",
            "2025-07-26 04:57:36,210 - INFO - Processing PCAR\n",
            "2025-07-26 04:57:36,220 - INFO - Loaded existing data for PCAR\n",
            "2025-07-26 04:57:36,223 - INFO - Processing ROST\n",
            "2025-07-26 04:57:36,232 - INFO - Loaded existing data for ROST\n",
            "Processing stocks:  63%|██████▎   | 62/98 [00:00<00:00, 84.28it/s]2025-07-26 04:57:36,236 - INFO - Processing CPRT\n",
            "2025-07-26 04:57:36,252 - INFO - Loaded existing data for CPRT\n",
            "2025-07-26 04:57:36,253 - INFO - Processing SIRI\n",
            "2025-07-26 04:57:36,261 - INFO - Loaded existing data for SIRI\n",
            "2025-07-26 04:57:36,262 - INFO - Processing EXC\n",
            "2025-07-26 04:57:36,272 - INFO - Loaded existing data for EXC\n",
            "2025-07-26 04:57:36,273 - INFO - Processing AEP\n",
            "2025-07-26 04:57:36,280 - INFO - Loaded existing data for AEP\n",
            "2025-07-26 04:57:36,281 - INFO - Processing VRSK\n",
            "2025-07-26 04:57:36,290 - INFO - Loaded existing data for VRSK\n",
            "2025-07-26 04:57:36,292 - INFO - Processing FAST\n",
            "2025-07-26 04:57:36,300 - INFO - Loaded existing data for FAST\n",
            "2025-07-26 04:57:36,302 - INFO - Processing DDOG\n",
            "2025-07-26 04:57:36,309 - INFO - Loaded existing data for DDOG\n",
            "2025-07-26 04:57:36,310 - INFO - Processing ANSS\n",
            "2025-07-26 04:57:36,318 - INFO - Loaded existing data for ANSS\n",
            "2025-07-26 04:57:36,319 - INFO - Processing DLTR\n",
            "2025-07-26 04:57:36,327 - INFO - Loaded existing data for DLTR\n",
            "2025-07-26 04:57:36,328 - INFO - Processing CTSH\n",
            "2025-07-26 04:57:36,338 - INFO - Loaded existing data for CTSH\n",
            "Processing stocks:  73%|███████▎  | 72/98 [00:00<00:00, 87.59it/s]2025-07-26 04:57:36,340 - INFO - Processing WBD\n",
            "2025-07-26 04:57:36,349 - INFO - Loaded existing data for WBD\n",
            "2025-07-26 04:57:36,351 - INFO - Processing FANG\n",
            "2025-07-26 04:57:36,360 - INFO - Loaded existing data for FANG\n",
            "2025-07-26 04:57:36,361 - INFO - Processing XEL\n",
            "2025-07-26 04:57:36,371 - INFO - Loaded existing data for XEL\n",
            "2025-07-26 04:57:36,373 - INFO - Processing TEAM\n",
            "2025-07-26 04:57:36,383 - INFO - Loaded existing data for TEAM\n",
            "2025-07-26 04:57:36,385 - INFO - Processing ILMN\n",
            "2025-07-26 04:57:36,397 - INFO - Loaded existing data for ILMN\n",
            "2025-07-26 04:57:36,398 - INFO - Processing EA\n",
            "2025-07-26 04:57:36,409 - INFO - Loaded existing data for EA\n",
            "2025-07-26 04:57:36,411 - INFO - Processing BKR\n",
            "2025-07-26 04:57:36,420 - INFO - Loaded existing data for BKR\n",
            "2025-07-26 04:57:36,421 - INFO - Processing ALGN\n",
            "2025-07-26 04:57:36,429 - INFO - Loaded existing data for ALGN\n",
            "2025-07-26 04:57:36,430 - INFO - Processing EBAY\n",
            "2025-07-26 04:57:36,438 - INFO - Loaded existing data for EBAY\n",
            "Processing stocks:  83%|████████▎ | 81/98 [00:00<00:00, 88.06it/s]2025-07-26 04:57:36,440 - INFO - Processing ZS\n",
            "2025-07-26 04:57:36,448 - INFO - Loaded existing data for ZS\n",
            "2025-07-26 04:57:36,449 - INFO - Processing CRWD\n",
            "2025-07-26 04:57:36,457 - INFO - Loaded existing data for CRWD\n",
            "2025-07-26 04:57:36,459 - INFO - Processing PYPL\n",
            "2025-07-26 04:57:36,468 - INFO - Loaded existing data for PYPL\n",
            "2025-07-26 04:57:36,469 - INFO - Processing CSX\n",
            "2025-07-26 04:57:36,478 - INFO - Loaded existing data for CSX\n",
            "2025-07-26 04:57:36,480 - INFO - Processing ATVI\n",
            "2025-07-26 04:57:36,491 - ERROR - $ATVI: possibly delisted; no timezone found\n",
            "2025-07-26 04:57:36,492 - WARNING - ATVI: No data returned from yfinance\n",
            "2025-07-26 04:57:36,493 - WARNING - Failed to process ATVI\n",
            "2025-07-26 04:57:36,494 - INFO - Processing ZM\n",
            "2025-07-26 04:57:36,504 - INFO - Loaded existing data for ZM\n",
            "2025-07-26 04:57:36,505 - INFO - Processing MTCH\n",
            "2025-07-26 04:57:36,514 - INFO - Loaded existing data for MTCH\n",
            "2025-07-26 04:57:36,516 - INFO - Processing ENPH\n",
            "2025-07-26 04:57:36,528 - INFO - Loaded existing data for ENPH\n",
            "2025-07-26 04:57:36,529 - INFO - Processing MRNA\n",
            "2025-07-26 04:57:36,544 - INFO - Loaded existing data for MRNA\n",
            "Processing stocks:  92%|█████████▏| 90/98 [00:01<00:00, 87.15it/s]2025-07-26 04:57:36,551 - INFO - Processing OKTA\n",
            "2025-07-26 04:57:36,563 - INFO - Loaded existing data for OKTA\n",
            "2025-07-26 04:57:36,566 - INFO - Processing DASH\n",
            "2025-07-26 04:57:36,575 - INFO - Loaded existing data for DASH\n",
            "2025-07-26 04:57:36,577 - INFO - Processing ROKU\n",
            "2025-07-26 04:57:36,585 - INFO - Loaded existing data for ROKU\n",
            "2025-07-26 04:57:36,586 - INFO - Processing TTD\n",
            "2025-07-26 04:57:36,598 - INFO - Loaded existing data for TTD\n",
            "2025-07-26 04:57:36,601 - INFO - Processing CVNA\n",
            "2025-07-26 04:57:36,612 - INFO - Loaded existing data for CVNA\n",
            "2025-07-26 04:57:36,613 - INFO - Processing RBLX\n",
            "2025-07-26 04:57:36,625 - INFO - Loaded existing data for RBLX\n",
            "2025-07-26 04:57:36,627 - INFO - Processing DOCU\n",
            "2025-07-26 04:57:36,636 - INFO - Loaded existing data for DOCU\n",
            "2025-07-26 04:57:36,638 - INFO - Processing SNAP\n",
            "2025-07-26 04:57:36,652 - INFO - Loaded existing data for SNAP\n",
            "Processing stocks: 100%|██████████| 98/98 [00:01<00:00, 83.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 98 tickers:\n",
            "- Successful: 97\n",
            "- Failed: 1\n",
            "\n",
            "Failed downloads:\n",
            "- ATVI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store individual dataframes\n",
        "stock_dfs = {}\n",
        "\n",
        "# Function to fetch and clean data for a single ticker\n",
        "def fetch_stock_data(ticker):\n",
        "    \"\"\"\n",
        "    Fetch and clean stock data for a given ticker.\n",
        "    \n",
        "    Args:\n",
        "        ticker (str): Stock ticker symbol\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame or None: Cleaned stock data if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch data\n",
        "        stock = yf.Ticker(ticker)\n",
        "        df = stock.history(\n",
        "            start=START_DATE,\n",
        "            end=END_DATE,\n",
        "            interval=INTERVAL\n",
        "        )\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            logging.warning(f\"{ticker}: No data returned from yfinance\")\n",
        "            return None\n",
        "            \n",
        "        # Reset index to make Date a column\n",
        "        df = df.reset_index()\n",
        "        \n",
        "        # Add ticker column\n",
        "        df['Ticker'] = ticker\n",
        "        \n",
        "        # Verify required columns\n",
        "        missing_cols = set(REQUIRED_COLUMNS) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            logging.warning(f\"{ticker}: Missing columns: {missing_cols}\")\n",
        "            return None\n",
        "            \n",
        "        # Clean data\n",
        "        df = df.dropna(subset=['Close', 'Volume'])  # Remove rows with missing critical data\n",
        "        \n",
        "        if len(df) < 100:  # Minimum data requirement\n",
        "            logging.warning(f\"{ticker}: Insufficient data points ({len(df)})\")\n",
        "            return None\n",
        "            \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Process each ticker\n",
        "for ticker in tqdm(tickers, desc=\"Processing stocks\"):\n",
        "    try:\n",
        "        logging.info(f\"Processing {ticker}\")\n",
        "        \n",
        "        # Check if file already exists\n",
        "        output_path = f'../data/stocks/{ticker}.csv'\n",
        "        if Path(output_path).exists():\n",
        "            # Load existing file\n",
        "            df = pd.read_csv(output_path)\n",
        "            stock_dfs[ticker] = df\n",
        "            logging.info(f\"Loaded existing data for {ticker}\")\n",
        "            continue\n",
        "        \n",
        "        # Fetch and process data\n",
        "        df = fetch_stock_data(ticker)\n",
        "        \n",
        "        if df is not None:\n",
        "            # Store in dictionary and save CSV\n",
        "            stock_dfs[ticker] = df\n",
        "            df.to_csv(output_path, index=False)\n",
        "            logging.info(f\"Saved {ticker} data with {len(df)} rows\")\n",
        "            print(f\"\\nSaved {output_path}\")\n",
        "            print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "            print(f\"Columns: {', '.join(df.columns)}\")\n",
        "        else:\n",
        "            logging.warning(f\"Failed to process {ticker}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Print summary\n",
        "successful = list(stock_dfs.keys())\n",
        "failed = [t for t in tickers if t not in successful]\n",
        "\n",
        "print(f\"\\nProcessed {len(tickers)} tickers:\")\n",
        "print(f\"- Successful: {len(successful)}\")\n",
        "print(f\"- Failed: {len(failed)}\")\n",
        "\n",
        "if failed:\n",
        "    print(\"\\nFailed downloads:\")\n",
        "    for ticker in failed:\n",
        "        print(f\"- {ticker}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data Collection Summary\n",
            "==================================================\n",
            "\n",
            "Total files generated: 97\n",
            "\n",
            "Sample Data Quality Check:\n",
            "\n",
            "AEP:\n",
            "- Rows: 2,651\n",
            "- Date Range: 2015-01-02 to 2025-07-18\n",
            "- Trading days per year: 251.4\n",
            "- Missing values: 0\n",
            "  ⚠️ Found 2651 rows with invalid prices\n",
            "\n",
            "GOOG:\n",
            "- Rows: 2,651\n",
            "- Date Range: 2015-01-02 to 2025-07-18\n",
            "- Trading days per year: 251.4\n",
            "- Missing values: 0\n",
            "  ⚠️ Found 2651 rows with invalid prices\n",
            "\n",
            "CDNS:\n",
            "- Rows: 2,651\n",
            "- Date Range: 2015-01-02 to 2025-07-18\n",
            "- Trading days per year: 251.4\n",
            "- Missing values: 0\n",
            "  ⚠️ Found 2651 rows with invalid prices\n",
            "\n",
            "DOCU:\n",
            "- Rows: 1,816\n",
            "- Date Range: 2018-04-27 to 2025-07-18\n",
            "- Trading days per year: 251.2\n",
            "- Missing values: 0\n",
            "  ⚠️ Found 1816 rows with invalid prices\n",
            "\n",
            "XEL:\n",
            "- Rows: 2,651\n",
            "- Date Range: 2015-01-02 to 2025-07-18\n",
            "- Trading days per year: 251.4\n",
            "- Missing values: 0\n",
            "  ⚠️ Found 2651 rows with invalid prices\n",
            "\n",
            "Overall Statistics:\n",
            "- Total trading days collected: 12,420\n",
            "- Date coverage: 2015-01-02 to 2025-07-18\n",
            "- Average rows per stock: 128.0\n",
            "\n",
            "Detailed logs saved to: ../reports/data_collection_20250726_045735.log\n",
            "\n",
            "Data collection complete!\n"
          ]
        }
      ],
      "source": [
        "# Data Collection Summary\n",
        "print(\"\\nData Collection Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyze downloaded files\n",
        "stock_files = list(Path('../data/stocks').glob('*.csv'))\n",
        "total_rows = 0\n",
        "date_ranges = []\n",
        "\n",
        "print(f\"\\nTotal files generated: {len(stock_files)}\")\n",
        "\n",
        "# Sample a few files to show data quality\n",
        "sample_files = np.random.choice(stock_files, min(5, len(stock_files)), replace=False)\n",
        "print(\"\\nSample Data Quality Check:\")\n",
        "\n",
        "for file in sample_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        ticker = file.stem\n",
        "        \n",
        "        # Convert date with UTC=True to avoid warning\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        \n",
        "        # Calculate basic statistics\n",
        "        print(f\"\\n{ticker}:\")\n",
        "        print(f\"- Rows: {len(df):,}\")\n",
        "        print(f\"- Date Range: {df['Date'].min():%Y-%m-%d} to {df['Date'].max():%Y-%m-%d}\")\n",
        "        print(f\"- Trading days per year: {len(df) / ((df['Date'].max() - df['Date'].min()).days / 365):.1f}\")\n",
        "        print(f\"- Missing values: {df.isnull().sum().sum():,}\")\n",
        "        \n",
        "        # Check data quality\n",
        "        price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "        if all(col in df.columns for col in price_cols):\n",
        "            invalid_prices = df[df[price_cols] <= 0].shape[0]\n",
        "            if invalid_prices > 0:\n",
        "                print(f\"  ⚠️ Found {invalid_prices} rows with invalid prices\")\n",
        "        \n",
        "        total_rows += len(df)\n",
        "        date_ranges.append((df['Date'].min(), df['Date'].max()))\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError reading {ticker}: {str(e)}\")\n",
        "\n",
        "if date_ranges:\n",
        "    overall_start = min(date[0] for date in date_ranges)\n",
        "    overall_end = max(date[1] for date in date_ranges)\n",
        "    \n",
        "    print(f\"\\nOverall Statistics:\")\n",
        "    print(f\"- Total trading days collected: {total_rows:,}\")\n",
        "    print(f\"- Date coverage: {overall_start:%Y-%m-%d} to {overall_end:%Y-%m-%d}\")\n",
        "    print(f\"- Average rows per stock: {total_rows / len(stock_files):,.1f}\")\n",
        "    \n",
        "print(f\"\\nDetailed logs saved to: {log_file}\")\n",
        "print(\"\\nData collection complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
