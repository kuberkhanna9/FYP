{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NASDAQ-100 Stock Data Collection and Validation\n",
        "\n",
        "This notebook downloads and validates data for NASDAQ-100 stocks with robust data quality checks.\n",
        "\n",
        "## Features\n",
        "- Downloads daily OHLCV data for NASDAQ-100 stocks\n",
        "- Implements comprehensive data quality validation\n",
        "- Handles outliers and data anomalies\n",
        "- Ensures consistent history across stocks\n",
        "- Validates against sector mappings\n",
        "\n",
        "## Data Quality Checks\n",
        "1. Completeness: Minimum history requirements\n",
        "2. Consistency: Price and volume validation\n",
        "3. Outlier Detection: IQR-based outlier identification\n",
        "4. Timeline Validation: No future dates\n",
        "5. Sector Coverage: Validation against sector mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:01:15,648 - INFO - Data collection configured for period: 2015-01-01 to 2025-07-26\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs will be saved to: ../reports/data_quality_20250726_070115.log\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import pytz\n",
        "\n",
        "# Configure logging\n",
        "log_file = f'../reports/data_quality_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create necessary directories\n",
        "for dir_path in ['../data/stocks', '../data/processed', '../reports']:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define parameters\n",
        "START_DATE = '2015-01-01'\n",
        "END_DATE = date.today().strftime('%Y-%m-%d')\n",
        "INTERVAL = '1d'\n",
        "MIN_HISTORY_DAYS = 1000\n",
        "REQUIRED_COLUMNS = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Set timezone for consistency\n",
        "UTC = pytz.UTC\n",
        "\n",
        "logging.info(f\"Data collection configured for period: {START_DATE} to {END_DATE}\")\n",
        "print(f\"Logs will be saved to: {log_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data quality functions\n",
        "def detect_outliers(df, columns=['Open', 'High', 'Low', 'Close'], threshold=3.0):  # Increased threshold\n",
        "    \"\"\"\n",
        "    Detect outliers using IQR method with a more lenient threshold.\n",
        "    Returns dictionary with outlier counts per column.\n",
        "    \"\"\"\n",
        "    outliers = {}\n",
        "    for col in columns:\n",
        "        # Calculate rolling median and IQR to handle trends\n",
        "        rolling_median = df[col].rolling(window=20, center=True).median()\n",
        "        Q1 = df[col].rolling(window=20, center=True).quantile(0.25)\n",
        "        Q3 = df[col].rolling(window=20, center=True).quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        # Handle zero/small IQR cases\n",
        "        IQR = np.where(IQR < 1e-10, df[col].std(), IQR)\n",
        "        \n",
        "        lower = rolling_median - threshold * IQR\n",
        "        upper = rolling_median + threshold * IQR\n",
        "        \n",
        "        outliers[col] = len(df[(df[col] < lower) | (df[col] > upper)])\n",
        "    return outliers\n",
        "\n",
        "def validate_price_data(df):\n",
        "    \"\"\"\n",
        "    Validate price data for common issues.\n",
        "    Returns list of validation issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    # Check for negative prices\n",
        "    if (df[['Open', 'High', 'Low', 'Close']] < 0).any().any():  # Changed from <= 0 to < 0\n",
        "        issues.append(\"Found negative prices\")\n",
        "    \n",
        "    # Check price relationships with tolerance\n",
        "    tolerance = df['Close'].rolling(window=20).std() * 0.01  # 1% of rolling std as tolerance\n",
        "    tolerance = tolerance.fillna(df['Close'].std() * 0.01)\n",
        "    \n",
        "    invalid_prices = (\n",
        "        (df['High'] < df['Low'] - tolerance) |\n",
        "        (df['Open'] > df['High'] + tolerance) |\n",
        "        (df['Open'] < df['Low'] - tolerance) |\n",
        "        (df['Close'] > df['High'] + tolerance) |\n",
        "        (df['Close'] < df['Low'] - tolerance)\n",
        "    )\n",
        "    if invalid_prices.any():\n",
        "        issues.append(f\"Found {invalid_prices.sum()} invalid price relationships\")\n",
        "    \n",
        "    # Check for extreme price changes with adaptive threshold\n",
        "    daily_returns = df['Close'].pct_change()\n",
        "    rolling_std = daily_returns.rolling(window=20).std()\n",
        "    threshold = rolling_std * 5  # 5 standard deviations\n",
        "    threshold = threshold.fillna(daily_returns.std() * 5)\n",
        "    extreme_returns = (daily_returns.abs() > threshold)\n",
        "    if extreme_returns.any():\n",
        "        issues.append(f\"Found {extreme_returns.sum()} extreme price changes\")\n",
        "    \n",
        "    return issues\n",
        "\n",
        "def handle_outliers(df, columns=['Open', 'High', 'Low', 'Close']):\n",
        "    \"\"\"\n",
        "    Handle outliers using Winsorization with adaptive thresholds.\n",
        "    Returns a new DataFrame with outliers handled.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    for col in columns:\n",
        "        # Calculate rolling statistics\n",
        "        rolling_median = df[col].rolling(window=20, center=True).median()\n",
        "        rolling_std = df[col].rolling(window=20, center=True).std()\n",
        "        \n",
        "        # Fill NaN values with overall statistics\n",
        "        rolling_median = rolling_median.fillna(df[col].median())\n",
        "        rolling_std = rolling_std.fillna(df[col].std())\n",
        "        \n",
        "        # Set threshold as 4 standard deviations\n",
        "        lower = rolling_median - 4 * rolling_std\n",
        "        upper = rolling_median + 4 * rolling_std\n",
        "        \n",
        "        # Winsorize the data\n",
        "        df_clean[col] = df[col].clip(lower=lower, upper=upper)\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "def validate_stock_data(df, ticker, sector):\n",
        "    \"\"\"\n",
        "    Comprehensive data validation for a single stock.\n",
        "    Returns tuple of (is_valid, issues).\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    # Check data completeness\n",
        "    if len(df) < MIN_HISTORY_DAYS:\n",
        "        issues.append(f\"Insufficient history: {len(df)} days < {MIN_HISTORY_DAYS}\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    if missing_values.any():\n",
        "        issues.append(f\"Missing values: {dict(missing_values[missing_values > 0])}\")\n",
        "    \n",
        "    # Check for future dates\n",
        "    current_date = pd.Timestamp(END_DATE, tz=UTC)\n",
        "    if df['Date'].max() > current_date:\n",
        "        issues.append(f\"Found future dates: {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Detect outliers (only consider it an issue if more than 5% of data points are outliers)\n",
        "    outliers = detect_outliers(df)\n",
        "    significant_outliers = {k: v for k, v in outliers.items() if v > len(df) * 0.05}\n",
        "    if significant_outliers:\n",
        "        for col, count in significant_outliers.items():\n",
        "            issues.append(f\"{count} potential outliers in {col} prices\")\n",
        "    \n",
        "    # Validate price data\n",
        "    price_issues = validate_price_data(df)\n",
        "    issues.extend(price_issues)\n",
        "    \n",
        "    # Only consider validation failed if there are serious issues\n",
        "    serious_issues = [\n",
        "        issue for issue in issues \n",
        "        if \"Missing values\" in issue \n",
        "        or \"Insufficient history\" in issue \n",
        "        or \"future dates\" in issue\n",
        "    ]\n",
        "    \n",
        "    return len(serious_issues) == 0, issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to fetch and clean data for a single ticker\n",
        "def fetch_stock_data(ticker, sector):\n",
        "    \"\"\"\n",
        "    Fetch and clean stock data for a given ticker.\n",
        "    \n",
        "    Args:\n",
        "        ticker (str): Stock ticker symbol\n",
        "        sector (str): Stock sector\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame or None: Cleaned stock data if successful, None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        output_path = f'../data/stocks/{ticker}.csv'\n",
        "        \n",
        "        # Check if file already exists\n",
        "        if Path(output_path).exists():\n",
        "            df = pd.read_csv(output_path)\n",
        "            df['Date'] = pd.to_datetime(df['Date'], utc=True)  # Properly handle timezone\n",
        "            logging.info(f\"Loaded existing data for {ticker}\")\n",
        "        else:\n",
        "            logging.info(f\"Downloading data for {ticker} ({sector})\")\n",
        "            # Fetch data\n",
        "            stock = yf.Ticker(ticker)\n",
        "            df = stock.history(\n",
        "                start=START_DATE,\n",
        "                end=END_DATE,\n",
        "                interval=INTERVAL\n",
        "            )\n",
        "            \n",
        "            if len(df) == 0:\n",
        "                logging.error(f\"No data downloaded for {ticker}\")\n",
        "                return None\n",
        "                \n",
        "            # Reset index to make Date a column\n",
        "            df = df.reset_index()\n",
        "            df['Date'] = pd.to_datetime(df['Date'], utc=True)  # Ensure timezone awareness\n",
        "            \n",
        "        # Add ticker and sector columns\n",
        "        df['Ticker'] = ticker\n",
        "        df['Sector'] = sector\n",
        "        \n",
        "        # Verify required columns\n",
        "        missing_cols = set(REQUIRED_COLUMNS) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            logging.warning(f\"{ticker}: Missing columns: {missing_cols}\")\n",
        "            return None\n",
        "            \n",
        "        # Clean data\n",
        "        df = df.dropna(subset=['Close', 'Volume'])  # Remove rows with missing critical data\n",
        "        \n",
        "        # Validate data\n",
        "        is_valid, issues = validate_stock_data(df, ticker, sector)\n",
        "        \n",
        "        if not is_valid:\n",
        "            for issue in issues:\n",
        "                logging.warning(f\"{ticker} data quality issues:\")\n",
        "                logging.warning(f\"- {issue}\")\n",
        "            \n",
        "            # Handle outliers if they exist\n",
        "            if any(\"outliers\" in issue for issue in issues):\n",
        "                df = handle_outliers(df)\n",
        "                logging.info(f\"Applied outlier handling for {ticker}\")\n",
        "            \n",
        "            validation_results['failed'].append(ticker)\n",
        "        else:\n",
        "            logging.info(f\"Validated {ticker} data: {len(df)} rows\")\n",
        "            validation_results['passed'].append(ticker)\n",
        "            \n",
        "        # Save processed data\n",
        "        df.to_csv(output_path, index=False)\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "        validation_results['excluded'].append(ticker)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:01:15,780 - INFO - Loaded 109 tickers from sector mapping\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sector distribution:\n",
            "- Information Technology: 52 stocks\n",
            "- Consumer Discretionary: 14 stocks\n",
            "- Healthcare: 11 stocks\n",
            "- Industrials: 7 stocks\n",
            "- Consumer Staples: 6 stocks\n",
            "- Communication Services: 6 stocks\n",
            "- Utilities: 3 stocks\n",
            "- Materials: 3 stocks\n",
            "- Real Estate: 3 stocks\n",
            "- Financials: 2 stocks\n",
            "- Energy: 2 stocks\n"
          ]
        }
      ],
      "source": [
        "# Load sector mapping and initialize results tracking\n",
        "try:\n",
        "    # Define GICS sectors and their ETFs\n",
        "    SECTOR_MAP = {\n",
        "        'XLK': 'Information Technology',\n",
        "        'XLF': 'Financials',\n",
        "        'XLV': 'Healthcare',\n",
        "        'XLE': 'Energy',\n",
        "        'XLY': 'Consumer Discretionary',\n",
        "        'XLP': 'Consumer Staples',\n",
        "        'XLI': 'Industrials',\n",
        "        'XLB': 'Materials',\n",
        "        'XLU': 'Utilities',\n",
        "        'XLRE': 'Real Estate',\n",
        "        'XLC': 'Communication Services'\n",
        "    }\n",
        "    \n",
        "    # Load and clean sector mapping\n",
        "    sector_mapping = pd.read_csv('../data/sector_mapping.csv')\n",
        "    \n",
        "    # Standardize sector names\n",
        "    sector_name_map = {\n",
        "        'Technology': 'Information Technology',\n",
        "        'Technology ': 'Information Technology',\n",
        "        'Consumer_Discretionary': 'Consumer Discretionary',\n",
        "        'Consumer_Staples': 'Consumer Staples',\n",
        "        'Communication_Services': 'Communication Services'\n",
        "    }\n",
        "    \n",
        "    # Clean sector names\n",
        "    sector_mapping['Sector'] = sector_mapping['Sector'].str.strip()\n",
        "    sector_mapping['Sector'] = sector_mapping['Sector'].replace(sector_name_map)\n",
        "    \n",
        "    # Get unique tickers\n",
        "    tickers = sector_mapping['Ticker'].unique()\n",
        "    logging.info(f\"Loaded {len(tickers)} tickers from sector mapping\")\n",
        "    \n",
        "    print(\"\\nSector distribution:\")\n",
        "    sector_counts = sector_mapping['Sector'].value_counts()\n",
        "    for sector, count in sector_counts.items():\n",
        "        print(f\"- {sector}: {count} stocks\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading sector mapping: {str(e)}\")\n",
        "    raise Exception(\"Cannot proceed without sector mapping\")\n",
        "\n",
        "# Dictionary to store validation results\n",
        "validation_results = {\n",
        "    'passed': [],\n",
        "    'failed': [],\n",
        "    'excluded': []\n",
        "}\n",
        "\n",
        "# Dictionary to store processed data\n",
        "stock_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing stocks:   0%|          | 0/109 [00:00<?, ?it/s]2025-07-26 07:01:15,843 - INFO - Loaded existing data for AAPL\n",
            "2025-07-26 07:01:15,888 - INFO - Validated AAPL data: 2651 rows\n",
            "Processing stocks:   1%|          | 1/109 [00:00<00:12,  8.55it/s]2025-07-26 07:01:15,944 - INFO - Loaded existing data for MSFT\n",
            "2025-07-26 07:01:15,973 - INFO - Validated MSFT data: 2651 rows\n",
            "2025-07-26 07:01:16,013 - INFO - Loaded existing data for AMZN\n",
            "2025-07-26 07:01:16,038 - INFO - Validated AMZN data: 2651 rows\n",
            "Processing stocks:   3%|▎         | 3/109 [00:00<00:08, 12.17it/s]2025-07-26 07:01:16,078 - INFO - Loaded existing data for NVDA\n",
            "2025-07-26 07:01:16,103 - INFO - Validated NVDA data: 2651 rows\n",
            "2025-07-26 07:01:16,141 - INFO - Loaded existing data for META\n",
            "2025-07-26 07:01:16,169 - INFO - Validated META data: 2651 rows\n",
            "Processing stocks:   5%|▍         | 5/109 [00:00<00:08, 12.26it/s]2025-07-26 07:01:16,265 - INFO - Loaded existing data for GOOGL\n",
            "2025-07-26 07:01:16,312 - INFO - Validated GOOGL data: 2651 rows\n",
            "2025-07-26 07:01:16,353 - INFO - Loaded existing data for GOOG\n",
            "2025-07-26 07:01:16,383 - INFO - Validated GOOG data: 2651 rows\n",
            "Processing stocks:   6%|▋         | 7/109 [00:00<00:09, 11.32it/s]2025-07-26 07:01:16,438 - INFO - Loaded existing data for TSLA\n",
            "2025-07-26 07:01:16,487 - INFO - Validated TSLA data: 2651 rows\n",
            "2025-07-26 07:01:16,540 - INFO - Loaded existing data for AVGO\n",
            "2025-07-26 07:01:16,583 - INFO - Validated AVGO data: 2651 rows\n",
            "Processing stocks:   8%|▊         | 9/109 [00:00<00:09, 10.74it/s]2025-07-26 07:01:16,651 - INFO - Loaded existing data for PEP\n",
            "2025-07-26 07:01:16,682 - INFO - Validated PEP data: 2651 rows\n",
            "2025-07-26 07:01:16,753 - INFO - Loaded existing data for COST\n",
            "2025-07-26 07:01:16,786 - INFO - Validated COST data: 2651 rows\n",
            "Processing stocks:  10%|█         | 11/109 [00:01<00:09, 10.19it/s]2025-07-26 07:01:16,869 - INFO - Loaded existing data for CSCO\n",
            "2025-07-26 07:01:16,907 - INFO - Validated CSCO data: 2651 rows\n",
            "2025-07-26 07:01:16,951 - INFO - Loaded existing data for TMUS\n",
            "2025-07-26 07:01:16,976 - INFO - Validated TMUS data: 2651 rows\n",
            "Processing stocks:  12%|█▏        | 13/109 [00:01<00:08, 10.76it/s]2025-07-26 07:01:17,015 - INFO - Loaded existing data for ADBE\n",
            "2025-07-26 07:01:17,048 - INFO - Validated ADBE data: 2651 rows\n",
            "2025-07-26 07:01:17,092 - INFO - Loaded existing data for NFLX\n",
            "2025-07-26 07:01:17,128 - INFO - Validated NFLX data: 2651 rows\n",
            "Processing stocks:  14%|█▍        | 15/109 [00:01<00:08, 11.39it/s]2025-07-26 07:01:17,174 - INFO - Loaded existing data for CMCSA\n",
            "2025-07-26 07:01:17,236 - INFO - Validated CMCSA data: 2651 rows\n",
            "2025-07-26 07:01:17,289 - INFO - Loaded existing data for AMD\n",
            "2025-07-26 07:01:17,321 - INFO - Validated AMD data: 2651 rows\n",
            "Processing stocks:  16%|█▌        | 17/109 [00:01<00:08, 11.08it/s]2025-07-26 07:01:17,362 - INFO - Loaded existing data for TXN\n",
            "2025-07-26 07:01:17,392 - INFO - Validated TXN data: 2651 rows\n",
            "2025-07-26 07:01:17,446 - INFO - Loaded existing data for INTC\n",
            "2025-07-26 07:01:17,488 - INFO - Validated INTC data: 2651 rows\n",
            "Processing stocks:  17%|█▋        | 19/109 [00:01<00:07, 11.31it/s]2025-07-26 07:01:17,529 - INFO - Loaded existing data for QCOM\n",
            "2025-07-26 07:01:17,558 - INFO - Validated QCOM data: 2651 rows\n",
            "2025-07-26 07:01:17,617 - INFO - Loaded existing data for INTU\n",
            "2025-07-26 07:01:17,648 - INFO - Validated INTU data: 2651 rows\n",
            "Processing stocks:  19%|█▉        | 21/109 [00:01<00:07, 11.52it/s]2025-07-26 07:01:17,717 - INFO - Loaded existing data for AMGN\n",
            "2025-07-26 07:01:17,769 - INFO - Validated AMGN data: 2651 rows\n",
            "2025-07-26 07:01:17,848 - INFO - Loaded existing data for HON\n",
            "2025-07-26 07:01:17,906 - INFO - Validated HON data: 2651 rows\n",
            "Processing stocks:  21%|██        | 23/109 [00:02<00:08, 10.12it/s]2025-07-26 07:01:17,952 - INFO - Loaded existing data for AMAT\n",
            "2025-07-26 07:01:18,014 - INFO - Validated AMAT data: 2651 rows\n",
            "2025-07-26 07:01:18,054 - INFO - Loaded existing data for ISRG\n",
            "2025-07-26 07:01:18,079 - INFO - Validated ISRG data: 2651 rows\n",
            "Processing stocks:  23%|██▎       | 25/109 [00:02<00:07, 10.62it/s]2025-07-26 07:01:18,115 - INFO - Loaded existing data for SBUX\n",
            "2025-07-26 07:01:18,146 - INFO - Validated SBUX data: 2651 rows\n",
            "2025-07-26 07:01:18,192 - INFO - Loaded existing data for ADP\n",
            "2025-07-26 07:01:18,235 - INFO - Validated ADP data: 2651 rows\n",
            "Processing stocks:  25%|██▍       | 27/109 [00:02<00:07, 10.96it/s]2025-07-26 07:01:18,294 - INFO - Loaded existing data for MDLZ\n",
            "2025-07-26 07:01:18,326 - INFO - Validated MDLZ data: 2651 rows\n",
            "2025-07-26 07:01:18,378 - INFO - Loaded existing data for ADI\n",
            "2025-07-26 07:01:18,424 - INFO - Validated ADI data: 2651 rows\n",
            "Processing stocks:  27%|██▋       | 29/109 [00:02<00:07, 10.88it/s]2025-07-26 07:01:18,480 - INFO - Loaded existing data for BKNG\n",
            "2025-07-26 07:01:18,518 - INFO - Validated BKNG data: 2651 rows\n",
            "2025-07-26 07:01:18,562 - INFO - Loaded existing data for GILD\n",
            "2025-07-26 07:01:18,609 - INFO - Validated GILD data: 2651 rows\n",
            "Processing stocks:  28%|██▊       | 31/109 [00:02<00:07, 10.30it/s]2025-07-26 07:01:18,714 - INFO - Loaded existing data for REGN\n",
            "2025-07-26 07:01:18,753 - INFO - Validated REGN data: 2651 rows\n",
            "2025-07-26 07:01:18,807 - INFO - Loaded existing data for VRTX\n",
            "2025-07-26 07:01:18,843 - INFO - Validated VRTX data: 2651 rows\n",
            "Processing stocks:  30%|███       | 33/109 [00:03<00:07, 10.20it/s]2025-07-26 07:01:18,899 - INFO - Loaded existing data for PANW\n",
            "2025-07-26 07:01:18,932 - INFO - Validated PANW data: 2651 rows\n",
            "2025-07-26 07:01:18,984 - INFO - Loaded existing data for SNPS\n",
            "2025-07-26 07:01:19,008 - INFO - Validated SNPS data: 2651 rows\n",
            "Processing stocks:  32%|███▏      | 35/109 [00:03<00:06, 10.85it/s]2025-07-26 07:01:19,046 - INFO - Loaded existing data for CDNS\n",
            "2025-07-26 07:01:19,072 - INFO - Validated CDNS data: 2651 rows\n",
            "2025-07-26 07:01:19,110 - INFO - Loaded existing data for KLAC\n",
            "2025-07-26 07:01:19,142 - INFO - Validated KLAC data: 2651 rows\n",
            "Processing stocks:  34%|███▍      | 37/109 [00:03<00:06, 11.69it/s]2025-07-26 07:01:19,187 - INFO - Loaded existing data for MELI\n",
            "2025-07-26 07:01:19,234 - INFO - Validated MELI data: 2651 rows\n",
            "2025-07-26 07:01:19,299 - INFO - Loaded existing data for LRCX\n",
            "2025-07-26 07:01:19,344 - INFO - Validated LRCX data: 2651 rows\n",
            "Processing stocks:  36%|███▌      | 39/109 [00:03<00:06, 11.02it/s]2025-07-26 07:01:19,397 - INFO - Loaded existing data for MU\n",
            "2025-07-26 07:01:19,435 - INFO - Validated MU data: 2651 rows\n",
            "2025-07-26 07:01:19,477 - INFO - Loaded existing data for ASML\n",
            "2025-07-26 07:01:19,503 - INFO - Validated ASML data: 2651 rows\n",
            "Processing stocks:  38%|███▊      | 41/109 [00:03<00:05, 11.67it/s]2025-07-26 07:01:19,540 - INFO - Loaded existing data for CHTR\n",
            "2025-07-26 07:01:19,566 - INFO - Validated CHTR data: 2651 rows\n",
            "2025-07-26 07:01:19,608 - INFO - Loaded existing data for MAR\n",
            "2025-07-26 07:01:19,645 - INFO - Validated MAR data: 2651 rows\n",
            "Processing stocks:  39%|███▉      | 43/109 [00:03<00:05, 12.05it/s]2025-07-26 07:01:19,702 - INFO - Loaded existing data for ORLY\n",
            "2025-07-26 07:01:19,727 - INFO - Validated ORLY data: 2651 rows\n",
            "2025-07-26 07:01:19,768 - INFO - Loaded existing data for MNST\n",
            "2025-07-26 07:01:19,800 - INFO - Validated MNST data: 2651 rows\n",
            "Processing stocks:  41%|████▏     | 45/109 [00:04<00:05, 12.39it/s]2025-07-26 07:01:19,844 - INFO - Loaded existing data for FTNT\n",
            "2025-07-26 07:01:19,868 - INFO - Validated FTNT data: 2651 rows\n",
            "2025-07-26 07:01:19,910 - INFO - Loaded existing data for CTAS\n",
            "2025-07-26 07:01:19,940 - INFO - Validated CTAS data: 2651 rows\n",
            "Processing stocks:  43%|████▎     | 47/109 [00:04<00:04, 13.03it/s]2025-07-26 07:01:19,978 - INFO - Loaded existing data for PAYX\n",
            "2025-07-26 07:01:20,001 - INFO - Validated PAYX data: 2651 rows\n",
            "2025-07-26 07:01:20,038 - INFO - Loaded existing data for MRVL\n",
            "2025-07-26 07:01:20,063 - INFO - Validated MRVL data: 2651 rows\n",
            "Processing stocks:  45%|████▍     | 49/109 [00:04<00:04, 13.87it/s]2025-07-26 07:01:20,100 - INFO - Loaded existing data for ABNB\n",
            "2025-07-26 07:01:20,115 - INFO - Validated ABNB data: 1155 rows\n",
            "2025-07-26 07:01:20,185 - INFO - Loaded existing data for ADSK\n",
            "2025-07-26 07:01:20,279 - INFO - Validated ADSK data: 2651 rows\n",
            "Processing stocks:  47%|████▋     | 51/109 [00:04<00:04, 11.64it/s]2025-07-26 07:01:20,342 - INFO - Loaded existing data for ODFL\n",
            "2025-07-26 07:01:20,377 - INFO - Validated ODFL data: 2651 rows\n",
            "2025-07-26 07:01:20,431 - INFO - Loaded existing data for IDXX\n",
            "2025-07-26 07:01:20,484 - INFO - Validated IDXX data: 2651 rows\n",
            "Processing stocks:  49%|████▊     | 53/109 [00:04<00:04, 11.31it/s]2025-07-26 07:01:20,525 - INFO - Loaded existing data for BIIB\n",
            "2025-07-26 07:01:20,549 - INFO - Validated BIIB data: 2651 rows\n",
            "2025-07-26 07:01:20,588 - INFO - Loaded existing data for KDP\n",
            "2025-07-26 07:01:20,614 - INFO - Validated KDP data: 2651 rows\n",
            "Processing stocks:  50%|█████     | 55/109 [00:04<00:04, 12.37it/s]2025-07-26 07:01:20,653 - INFO - Loaded existing data for KHC\n",
            "2025-07-26 07:01:20,678 - INFO - Validated KHC data: 2525 rows\n",
            "2025-07-26 07:01:20,715 - INFO - Loaded existing data for DXCM\n",
            "2025-07-26 07:01:20,745 - INFO - Validated DXCM data: 2651 rows\n",
            "Processing stocks:  52%|█████▏    | 57/109 [00:04<00:04, 12.94it/s]2025-07-26 07:01:20,810 - INFO - Loaded existing data for NXPI\n",
            "2025-07-26 07:01:20,848 - INFO - Validated NXPI data: 2651 rows\n",
            "2025-07-26 07:01:20,886 - INFO - Loaded existing data for MCHP\n",
            "2025-07-26 07:01:20,908 - INFO - Validated MCHP data: 2651 rows\n",
            "Processing stocks:  54%|█████▍    | 59/109 [00:05<00:03, 12.91it/s]2025-07-26 07:01:20,947 - INFO - Loaded existing data for WDAY\n",
            "2025-07-26 07:01:20,972 - INFO - Validated WDAY data: 2651 rows\n",
            "2025-07-26 07:01:21,009 - INFO - Loaded existing data for PCAR\n",
            "2025-07-26 07:01:21,037 - INFO - Validated PCAR data: 2651 rows\n",
            "Processing stocks:  56%|█████▌    | 61/109 [00:05<00:03, 13.29it/s]2025-07-26 07:01:21,098 - INFO - Loaded existing data for ROST\n",
            "2025-07-26 07:01:21,160 - INFO - Validated ROST data: 2651 rows\n",
            "2025-07-26 07:01:21,227 - INFO - Loaded existing data for CPRT\n",
            "2025-07-26 07:01:21,251 - INFO - Validated CPRT data: 2651 rows\n",
            "Processing stocks:  58%|█████▊    | 63/109 [00:05<00:03, 12.04it/s]2025-07-26 07:01:21,288 - INFO - Loaded existing data for SIRI\n",
            "2025-07-26 07:01:21,317 - INFO - Validated SIRI data: 2651 rows\n",
            "2025-07-26 07:01:21,401 - INFO - Loaded existing data for EXC\n",
            "2025-07-26 07:01:21,453 - INFO - Validated EXC data: 2651 rows\n",
            "Processing stocks:  60%|█████▉    | 65/109 [00:05<00:04, 10.76it/s]2025-07-26 07:01:21,532 - INFO - Loaded existing data for AEP\n",
            "2025-07-26 07:01:21,586 - INFO - Validated AEP data: 2651 rows\n",
            "2025-07-26 07:01:21,620 - INFO - Loaded existing data for VRSK\n",
            "2025-07-26 07:01:21,646 - INFO - Validated VRSK data: 2651 rows\n",
            "Processing stocks:  61%|██████▏   | 67/109 [00:05<00:03, 11.15it/s]2025-07-26 07:01:21,686 - INFO - Loaded existing data for FAST\n",
            "2025-07-26 07:01:21,712 - INFO - Validated FAST data: 2651 rows\n",
            "2025-07-26 07:01:21,746 - INFO - Loaded existing data for DDOG\n",
            "2025-07-26 07:01:21,762 - INFO - Validated DDOG data: 1465 rows\n",
            "Processing stocks:  63%|██████▎   | 69/109 [00:05<00:03, 12.76it/s]2025-07-26 07:01:21,791 - INFO - Loaded existing data for ANSS\n",
            "2025-07-26 07:01:21,825 - INFO - Validated ANSS data: 2650 rows\n",
            "2025-07-26 07:01:21,862 - INFO - Loaded existing data for DLTR\n",
            "2025-07-26 07:01:21,895 - INFO - Validated DLTR data: 2651 rows\n",
            "Processing stocks:  65%|██████▌   | 71/109 [00:06<00:02, 13.05it/s]2025-07-26 07:01:21,935 - INFO - Loaded existing data for CTSH\n",
            "2025-07-26 07:01:21,974 - INFO - Validated CTSH data: 2651 rows\n",
            "2025-07-26 07:01:22,035 - INFO - Loaded existing data for WBD\n",
            "2025-07-26 07:01:22,073 - INFO - Validated WBD data: 2651 rows\n",
            "Processing stocks:  67%|██████▋   | 73/109 [00:06<00:02, 12.24it/s]2025-07-26 07:01:22,122 - INFO - Loaded existing data for FANG\n",
            "2025-07-26 07:01:22,181 - INFO - Validated FANG data: 2651 rows\n",
            "2025-07-26 07:01:22,237 - INFO - Loaded existing data for XEL\n",
            "2025-07-26 07:01:22,272 - INFO - Validated XEL data: 2651 rows\n",
            "Processing stocks:  69%|██████▉   | 75/109 [00:06<00:02, 11.57it/s]2025-07-26 07:01:22,318 - INFO - Loaded existing data for TEAM\n",
            "2025-07-26 07:01:22,378 - INFO - Validated TEAM data: 2415 rows\n",
            "2025-07-26 07:01:22,423 - INFO - Loaded existing data for ILMN\n",
            "2025-07-26 07:01:22,465 - INFO - Validated ILMN data: 2651 rows\n",
            "Processing stocks:  71%|███████   | 77/109 [00:06<00:02, 11.00it/s]2025-07-26 07:01:22,522 - INFO - Loaded existing data for EA\n",
            "2025-07-26 07:01:22,587 - INFO - Validated EA data: 2651 rows\n",
            "2025-07-26 07:01:22,632 - INFO - Loaded existing data for BKR\n",
            "2025-07-26 07:01:22,661 - INFO - Validated BKR data: 2651 rows\n",
            "Processing stocks:  72%|███████▏  | 79/109 [00:06<00:02, 11.07it/s]2025-07-26 07:01:22,697 - INFO - Loaded existing data for ALGN\n",
            "2025-07-26 07:01:22,723 - INFO - Validated ALGN data: 2651 rows\n",
            "2025-07-26 07:01:22,771 - INFO - Loaded existing data for EBAY\n",
            "2025-07-26 07:01:22,797 - INFO - Validated EBAY data: 2651 rows\n",
            "Processing stocks:  74%|███████▍  | 81/109 [00:07<00:02, 11.87it/s]2025-07-26 07:01:22,840 - INFO - Loaded existing data for ZS\n",
            "2025-07-26 07:01:22,877 - INFO - Validated ZS data: 1845 rows\n",
            "2025-07-26 07:01:22,928 - INFO - Loaded existing data for CRWD\n",
            "2025-07-26 07:01:22,966 - INFO - Validated CRWD data: 1534 rows\n",
            "Processing stocks:  76%|███████▌  | 83/109 [00:07<00:02, 12.07it/s]2025-07-26 07:01:22,997 - INFO - Loaded existing data for PYPL\n",
            "2025-07-26 07:01:23,032 - INFO - Validated PYPL data: 2525 rows\n",
            "2025-07-26 07:01:23,075 - INFO - Loaded existing data for CSX\n",
            "2025-07-26 07:01:23,113 - INFO - Validated CSX data: 2651 rows\n",
            "Processing stocks:  78%|███████▊  | 85/109 [00:07<00:01, 12.26it/s]2025-07-26 07:01:23,144 - INFO - Downloading data for ATVI (Information Technology)\n",
            "2025-07-26 07:01:24,814 - ERROR - $ATVI: possibly delisted; no timezone found\n",
            "2025-07-26 07:01:24,817 - ERROR - No data downloaded for ATVI\n",
            "2025-07-26 07:01:24,862 - INFO - Loaded existing data for ZM\n",
            "2025-07-26 07:01:24,918 - INFO - Validated ZM data: 1571 rows\n",
            "Processing stocks:  80%|███████▉  | 87/109 [00:09<00:07,  3.04it/s]2025-07-26 07:01:24,970 - INFO - Loaded existing data for MTCH\n",
            "2025-07-26 07:01:25,003 - INFO - Validated MTCH data: 2651 rows\n",
            "2025-07-26 07:01:25,057 - INFO - Loaded existing data for ENPH\n",
            "2025-07-26 07:01:25,089 - INFO - Validated ENPH data: 2651 rows\n",
            "Processing stocks:  82%|████████▏ | 89/109 [00:09<00:05,  3.92it/s]2025-07-26 07:01:25,128 - INFO - Loaded existing data for MRNA\n",
            "2025-07-26 07:01:25,151 - INFO - Validated MRNA data: 1661 rows\n",
            "2025-07-26 07:01:25,177 - INFO - Loaded existing data for OKTA\n",
            "2025-07-26 07:01:25,202 - INFO - Validated OKTA data: 2081 rows\n",
            "Processing stocks:  83%|████████▎ | 91/109 [00:09<00:03,  5.15it/s]2025-07-26 07:01:25,233 - INFO - Loaded existing data for DASH\n",
            "2025-07-26 07:01:25,251 - INFO - Validated DASH data: 1156 rows\n",
            "2025-07-26 07:01:25,279 - INFO - Loaded existing data for ROKU\n",
            "2025-07-26 07:01:25,307 - INFO - Validated ROKU data: 1961 rows\n",
            "Processing stocks:  85%|████████▌ | 93/109 [00:09<00:02,  6.57it/s]2025-07-26 07:01:25,347 - INFO - Loaded existing data for TTD\n",
            "2025-07-26 07:01:25,380 - INFO - Validated TTD data: 2218 rows\n",
            "2025-07-26 07:01:25,421 - INFO - Loaded existing data for CVNA\n",
            "2025-07-26 07:01:25,463 - INFO - Validated CVNA data: 2067 rows\n",
            "Processing stocks:  87%|████████▋ | 95/109 [00:09<00:01,  7.64it/s]2025-07-26 07:01:25,503 - INFO - Loaded existing data for RBLX\n",
            "2025-07-26 07:01:25,533 - INFO - Validated RBLX data: 1095 rows\n",
            "2025-07-26 07:01:25,572 - INFO - Loaded existing data for DOCU\n",
            "2025-07-26 07:01:25,597 - INFO - Validated DOCU data: 1816 rows\n",
            "Processing stocks:  89%|████████▉ | 97/109 [00:09<00:01,  9.11it/s]2025-07-26 07:01:25,631 - INFO - Loaded existing data for SNAP\n",
            "2025-07-26 07:01:25,655 - INFO - Validated SNAP data: 2107 rows\n",
            "2025-07-26 07:01:25,687 - INFO - Downloading data for GFS (Information Technology)\n",
            "2025-07-26 07:01:26,702 - WARNING - GFS data quality issues:\n",
            "2025-07-26 07:01:26,703 - WARNING - - Insufficient history: 938 days < 1000\n",
            "Processing stocks:  91%|█████████ | 99/109 [00:10<00:02,  4.13it/s]2025-07-26 07:01:26,721 - INFO - Downloading data for LCID (Consumer Discretionary)\n",
            "2025-07-26 07:01:27,302 - INFO - Validated LCID data: 1218 rows\n",
            "2025-07-26 07:01:27,344 - INFO - Downloading data for ON (Information Technology)\n",
            "2025-07-26 07:01:28,487 - INFO - Validated ON data: 2656 rows\n",
            "Processing stocks:  93%|█████████▎| 101/109 [00:12<00:03,  2.27it/s]2025-07-26 07:01:28,530 - INFO - Downloading data for RIVN (Consumer Discretionary)\n",
            "2025-07-26 07:01:28,962 - WARNING - RIVN data quality issues:\n",
            "2025-07-26 07:01:28,964 - WARNING - - Insufficient history: 929 days < 1000\n",
            "Processing stocks:  94%|█████████▎| 102/109 [00:13<00:03,  2.25it/s]2025-07-26 07:01:28,990 - INFO - Downloading data for COIN (Financials)\n",
            "2025-07-26 07:01:29,868 - INFO - Validated COIN data: 1076 rows\n",
            "Processing stocks:  94%|█████████▍| 103/109 [00:14<00:03,  1.86it/s]2025-07-26 07:01:29,897 - INFO - Downloading data for EQIX (Real Estate)\n",
            "2025-07-26 07:01:31,093 - INFO - Validated EQIX data: 2656 rows\n",
            "Processing stocks:  95%|█████████▌| 104/109 [00:15<00:03,  1.44it/s]2025-07-26 07:01:31,148 - INFO - Downloading data for PLD (Real Estate)\n",
            "2025-07-26 07:01:32,351 - INFO - Validated PLD data: 2656 rows\n",
            "Processing stocks:  96%|█████████▋| 105/109 [00:16<00:03,  1.20it/s]2025-07-26 07:01:32,401 - INFO - Downloading data for CSGP (Real Estate)\n",
            "2025-07-26 07:01:33,463 - INFO - Validated CSGP data: 2656 rows\n",
            "Processing stocks:  97%|█████████▋| 106/109 [00:17<00:02,  1.11it/s]2025-07-26 07:01:33,500 - INFO - Downloading data for FCX (Materials)\n",
            "2025-07-26 07:01:34,736 - INFO - Validated FCX data: 2656 rows\n",
            "Processing stocks:  98%|█████████▊| 107/109 [00:18<00:02,  1.01s/it]2025-07-26 07:01:34,792 - INFO - Downloading data for APD (Materials)\n",
            "2025-07-26 07:01:35,996 - INFO - Validated APD data: 2656 rows\n",
            "Processing stocks:  99%|█████████▉| 108/109 [00:20<00:01,  1.07s/it]2025-07-26 07:01:36,037 - INFO - Downloading data for LIN (Materials)\n",
            "2025-07-26 07:01:37,241 - INFO - Validated LIN data: 2656 rows\n",
            "Processing stocks: 100%|██████████| 109/109 [00:21<00:00,  5.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Summary:\n",
            "- Passed: 106 stocks\n",
            "- Failed: 2 stocks\n",
            "- Excluded: 0 stocks\n",
            "\n",
            "Failed validations:\n",
            "- GFS (Information Technology)\n",
            "- RIVN (Consumer Discretionary)\n",
            "\n",
            "Detailed validation report saved to: ../reports/data_quality_20250726_070115.log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process each ticker\n",
        "for ticker in tqdm(tickers, desc=\"Processing stocks\"):\n",
        "    try:\n",
        "        # Get sector for the ticker\n",
        "        sector = sector_mapping[sector_mapping['Ticker'] == ticker]['Sector'].iloc[0]\n",
        "        \n",
        "        # Fetch and process data\n",
        "        df = fetch_stock_data(ticker, sector)\n",
        "        \n",
        "        if df is not None:\n",
        "            stock_data[ticker] = df\n",
        "            \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {ticker}: {str(e)}\")\n",
        "        validation_results['excluded'].append(ticker)\n",
        "        continue\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nValidation Summary:\")\n",
        "print(f\"- Passed: {len(validation_results['passed'])} stocks\")\n",
        "print(f\"- Failed: {len(validation_results['failed'])} stocks\")\n",
        "print(f\"- Excluded: {len(validation_results['excluded'])} stocks\")\n",
        "\n",
        "if validation_results['failed']:\n",
        "    print(\"\\nFailed validations:\")\n",
        "    for ticker in validation_results['failed']:\n",
        "        sector = sector_mapping[sector_mapping['Ticker'] == ticker]['Sector'].iloc[0]\n",
        "        print(f\"- {ticker} ({sector})\")\n",
        "\n",
        "if validation_results['excluded']:\n",
        "    print(\"\\nExcluded stocks:\")\n",
        "    for ticker in validation_results['excluded']:\n",
        "        sector = sector_mapping[sector_mapping['Ticker'] == ticker]['Sector'].iloc[0]\n",
        "        print(f\"- {ticker} ({sector})\")\n",
        "\n",
        "print(f\"\\nDetailed validation report saved to: {log_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data Quality Metrics:\n",
            "Average trading days: 2463.2\n",
            "Date range: 2015-01-02 to 2025-07-25\n",
            "\n",
            "Sector-wise Statistics:\n",
            "                       Total_Days             Missing_Values\n",
            "                             mean   min   max            sum\n",
            "Sector                                                      \n",
            "Communication Services    2651.00  2651  2651              0\n",
            "Consumer Discretionary    2277.07   929  2651              0\n",
            "Consumer Staples          2630.00  2525  2651              0\n",
            "Energy                    2651.00  2651  2651              0\n",
            "Financials                1800.50  1076  2525              0\n",
            "Healthcare                2561.00  1661  2651              0\n",
            "Industrials               2651.00  2651  2651              0\n",
            "Information Technology    2410.67   938  2656              0\n",
            "Materials                 2656.00  2656  2656              0\n",
            "Real Estate               2656.00  2656  2656              0\n",
            "Utilities                 2651.00  2651  2651              0\n",
            "\n",
            "Data quality analysis complete!\n",
            "Detailed metrics saved to: ../data/processed/data_quality_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "# Generate data quality metrics\n",
        "def analyze_data_quality():\n",
        "    \"\"\"\n",
        "    Generate comprehensive data quality metrics.\n",
        "    \"\"\"\n",
        "    if not stock_data:\n",
        "        logging.error(\"No data available for quality analysis\")\n",
        "        return pd.DataFrame()\n",
        "        \n",
        "    quality_stats = []\n",
        "    \n",
        "    for ticker, df in stock_data.items():\n",
        "        try:\n",
        "            # Calculate basic statistics\n",
        "            stats = {\n",
        "                'Ticker': ticker,\n",
        "                'Sector': df['Sector'].iloc[0],\n",
        "                'Total_Days': len(df),\n",
        "                'Start_Date': df['Date'].min().strftime('%Y-%m-%d'),\n",
        "                'End_Date': df['Date'].max().strftime('%Y-%m-%d'),\n",
        "                'Missing_Values': df.isnull().sum().sum(),\n",
        "                'Trading_Days_Per_Year': len(df) / ((df['Date'].max() - df['Date'].min()).days / 365)\n",
        "            }\n",
        "            \n",
        "            # Calculate price statistics\n",
        "            for col in ['Open', 'High', 'Low', 'Close']:\n",
        "                stats[f'{col}_Mean'] = df[col].mean()\n",
        "                stats[f'{col}_Std'] = df[col].std()\n",
        "                stats[f'{col}_Min'] = df[col].min()\n",
        "                stats[f'{col}_Max'] = df[col].max()\n",
        "            \n",
        "            quality_stats.append(stats)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating metrics for {ticker}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if not quality_stats:\n",
        "        logging.error(\"No quality statistics generated\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    quality_df = pd.DataFrame(quality_stats)\n",
        "    \n",
        "    try:\n",
        "        # Save quality metrics\n",
        "        quality_df.to_csv('../data/processed/data_quality_metrics.csv', index=False)\n",
        "        \n",
        "        # Print summary statistics\n",
        "        print(\"\\nData Quality Metrics:\")\n",
        "        print(f\"Average trading days: {quality_df['Total_Days'].mean():.1f}\")\n",
        "        print(f\"Date range: {quality_df['Start_Date'].min()} to {quality_df['End_Date'].max()}\")\n",
        "        \n",
        "        # Analyze by sector\n",
        "        print(\"\\nSector-wise Statistics:\")\n",
        "        sector_stats = quality_df.groupby('Sector').agg({\n",
        "            'Total_Days': ['mean', 'min', 'max'],\n",
        "            'Missing_Values': 'sum'\n",
        "        }).round(2)\n",
        "        \n",
        "        print(sector_stats)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating quality metrics summary: {str(e)}\")\n",
        "    \n",
        "    return quality_df\n",
        "\n",
        "# Generate quality metrics\n",
        "quality_metrics = analyze_data_quality()\n",
        "\n",
        "if not quality_metrics.empty:\n",
        "    print(\"\\nData quality analysis complete!\")\n",
        "    print(\"Detailed metrics saved to: ../data/processed/data_quality_metrics.csv\")\n",
        "else:\n",
        "    print(\"\\nNo quality metrics generated due to data processing errors.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-26 07:01:42,043 - INFO - Saved merged dataset to ../data/processed/nasdaq_stocks_merged.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data collection and validation complete!\n",
            "- Merged dataset saved to: ../data/processed/nasdaq_stocks_merged.csv\n",
            "- Summary report saved to: ../reports/data_collection_summary.txt\n",
            "- Detailed log saved to: ../reports/data_quality_20250726_070115.log\n"
          ]
        }
      ],
      "source": [
        "# Save merged dataset\n",
        "if stock_data:\n",
        "    try:\n",
        "        # Concatenate all validated dataframes\n",
        "        merged_df = pd.concat(stock_data.values(), axis=0, ignore_index=True)\n",
        "        \n",
        "        # Sort by date, sector, and ticker\n",
        "        merged_df = merged_df.sort_values(['Date', 'Sector', 'Ticker'])\n",
        "        \n",
        "        # Save merged dataset\n",
        "        output_path = '../data/processed/nasdaq_stocks_merged.csv'\n",
        "        merged_df.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Saved merged dataset to {output_path}\")\n",
        "        \n",
        "        # Generate summary report\n",
        "        with open('../reports/data_collection_summary.txt', 'w') as f:\n",
        "            f.write(\"NASDAQ-100 Data Collection Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            \n",
        "            f.write(\"Data Coverage:\\n\")\n",
        "            f.write(f\"- Start Date: {merged_df['Date'].min().strftime('%Y-%m-%d')}\\n\")\n",
        "            f.write(f\"- End Date: {merged_df['Date'].max().strftime('%Y-%m-%d')}\\n\")\n",
        "            f.write(f\"- Total Trading Days: {len(merged_df['Date'].unique()):,}\\n\\n\")\n",
        "            \n",
        "            f.write(\"Stock Coverage:\\n\")\n",
        "            f.write(f\"- Total Stocks: {len(stock_data)}\\n\")\n",
        "            f.write(f\"- Passed Validation: {len(validation_results['passed'])}\\n\")\n",
        "            f.write(f\"- Failed Validation: {len(validation_results['failed'])}\\n\")\n",
        "            f.write(f\"- Excluded: {len(validation_results['excluded'])}\\n\\n\")\n",
        "            \n",
        "            f.write(\"Sector Coverage:\\n\")\n",
        "            for sector, count in sector_counts.items():\n",
        "                f.write(f\"- {sector}: {count} stocks\\n\")\n",
        "            \n",
        "        print(\"\\nData collection and validation complete!\")\n",
        "        print(f\"- Merged dataset saved to: {output_path}\")\n",
        "        print(\"- Summary report saved to: ../reports/data_collection_summary.txt\")\n",
        "        print(f\"- Detailed log saved to: {log_file}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving merged dataset: {str(e)}\")\n",
        "else:\n",
        "    logging.error(\"No validated data available to merge\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
